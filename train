# hybrid deterministic - probablistic model:

# WORK IN PROGRESS: v3 VERSION OF MOTOR STATE MODEL TRAINING with 4 OUTPUTS RATHER THAN 2 - May 2025 EN
# 30 frames

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import tf2onnx
import onnx
import h5py
import time
import scipy.io as sio
import pickle
import seaborn as sns
import random
import joblib
from tqdm import tqdm
import scipy.ndimage as ndi
import itertools 

from itertools import groupby

from sklearn.cluster import KMeans

import time 

from scipy.fft import fft
from scipy.fft import rfft, rfftfreq
from scipy.signal import butter, filtfilt, savgol_filter, correlate, find_peaks, medfilt, welch
import scipy.stats as stats
from scipy.stats import pointbiserialr, skew, kurtosis, spearmanr, mode, entropy
from functools import partial
import onnxruntime as ort
from scipy.ndimage import uniform_filter1d, maximum_filter1d, minimum_filter1d, generic_filter1d, median_filter, gaussian_filter1d, label, binary_dilation
from scipy.ndimage import binary_opening, binary_closing, binary_erosion

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, precision_recall_curve, ConfusionMatrixDisplay
)
from sklearn.preprocessing import OneHotEncoder, label_binarize, StandardScaler, RobustScaler, MinMaxScaler, robust_scale

from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import (
    Concatenate, GRU, MultiHeadAttention, Input, Bidirectional, 
    LSTM, BatchNormalization, Dropout, Dense, TimeDistributed, SpatialDropout1D, LayerNormalization, Conv1D, 
    Add, Attention, GlobalAveragePooling1D, RepeatVector, Reshape, Softmax, Multiply, GlobalMaxPooling1D
)

from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback

from tensorflow.keras.metrics import CategoricalAccuracy

from tensorflow.keras.optimizers.schedules import CosineDecay

import matplotlib.pyplot as plt
from collections import Counter, defaultdict

import tensorflow.keras.backend as K

from scipy.signal import argrelextrema

from tensorflow.keras.activations import swish

from tensorflow.keras.initializers import Constant

from tensorflow.keras.layers import Lambda

from tensorflow.keras.utils import to_categorical

from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy

from tensorflow.keras import regularizers



###############################################################################
#                                Utility Functions                            #
###############################################################################

def log_memory_usage(stage):
    """
    Log the current memory usage to help diagnose memory leaks.
    """
    import psutil
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f"{stage} - Memory usage: {mem_info.rss / (1024 ** 2):.2f} MB")

def convert_mat_to_hdf5(mat_file_path, hdf5_file_path):
    """
    Convert a .mat file to .hdf5 format.
    """
    mat_data = sio.loadmat(mat_file_path)
    with h5py.File(hdf5_file_path, 'w') as hdf5_file:
        for key, value in mat_data.items():
            if isinstance(value, np.ndarray):
                if np.issubdtype(value.dtype, np.number):
                    hdf5_file.create_dataset(key, data=value)
                else:
                    print(f"Skipping {key}: Non-numeric data (dtype {value.dtype})")
            else:
                print(f"Skipping {key}: Non-numeric or unsupported data type {type(value)}")
    print(f"Conversion complete. Data saved to {hdf5_file_path}.")  
    
    

    # Frame window for inspection
    start, end = 13000, 13500

    # Feature indices (adjust if needed)
    velocity_idx = 0
    cam_x_idx = 2
    cam_z_idx = 3

    # Slice out the window
    vel_slice = data[start:end, velocity_idx]
    cam_x_slice = data[start:end, cam_x_idx]
    cam_z_slice = data[start:end, cam_z_idx]

    # Plot the data
    plt.figure(figsize=(12, 6))
    plt.plot(vel_slice, label='Velocity XZ (Index 0)', color='cyan')
    plt.plot(cam_x_slice, label='cam_x (Index 2)', color='orange')
    plt.plot(cam_z_slice, label='cam_z (Index 3)', color='purple')
    plt.title(f"Raw Signal Inspection: Frames {start} to {end}")
    plt.xlabel("Frames")
    plt.ylabel("Value")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    
def detect_low_motion_regions(signal, window_size=5, var_thresh=1e-7):
    """
    Detects low-motion regions using rolling window variance, compatible with older NumPy.
    """
    N = len(signal)
    var_values = np.zeros(N)

    half = window_size // 2
    for i in range(N):
        start = max(i - half, 0)
        end = min(i + half + 1, N)
        var_values[i] = np.var(signal[start:end])

    return var_values < var_thresh


def find_flat_regions(signal, min_len=10):
    """
    Finds indices of flat regions where signal value doesn't change
    for at least `min_len` consecutive frames.
    """
    diffs = np.diff(signal, prepend=signal[0])
    flat_mask = np.abs(diffs) < 1e-8
    flat_regions = []

    count = 0
    for i, is_flat in enumerate(flat_mask):
        if is_flat:
            count += 1
        else:
            if count >= min_len:
                flat_regions.extend(range(i - count, i))
            count = 0
    if count >= min_len:
        flat_regions.extend(range(len(flat_mask) - count, len(flat_mask)))

    return np.array(flat_regions, dtype=int)

def find_flat_sequences(signal, threshold=0.002):
    """
    Finds continuous flat sequences in the signal where values stay below the given threshold.
    Returns a list of index arrays (segments).
    """
    below_thresh = np.abs(signal) < threshold
    segments = []
    start = None

    for i, is_flat in enumerate(below_thresh):
        if is_flat:
            if start is None:
                start = i
        else:
            if start is not None:
                segments.append(np.arange(start, i))
                start = None

    if start is not None:
        segments.append(np.arange(start, len(signal)))

    return segments


def clean_dropout_frames(data, labels_main, labels_fog,
                                   cam_x_idx=0, cam_z_idx=2, velocity_idx=0,
                                   pad=2, var_thresh=1e-7, window_size=5,
                                   min_flat_len=3, velocity_thresh=0.002, velocity_flat_len=12,
                                   sitting_class_index=0, fog_column_index=0, debug=True):
    """
    Detects and removes likely sensor dropout frames based on flat or low-variance motion,
    while preserving 'Sitting' and 'FOG' labeled frames to avoid harming model training.
    """
    cam_x = data[:, cam_x_idx]
    cam_z = data[:, cam_z_idx]
    velocity = data[:, velocity_idx]

    fog_mask = labels_fog[:, fog_column_index] == 1
    sitting_mask = np.argmax(labels_main, axis=1) == sitting_class_index

    low_var_mask = detect_low_motion_regions(cam_x, window_size, var_thresh) & \
                   detect_low_motion_regions(cam_z, window_size, var_thresh)
    flat_x = find_flat_regions(cam_x, min_len=min_flat_len)
    flat_z = find_flat_regions(cam_z, min_len=min_flat_len)

    flat_velocity_segments = find_flat_sequences(velocity, threshold=velocity_thresh)
    long_enough_segments = [seg for seg in flat_velocity_segments if len(seg) >= velocity_flat_len]
    flat_velocity = np.concatenate(long_enough_segments) if long_enough_segments else np.array([], dtype=int)
    low_var_velocity_indices = np.where(detect_low_motion_regions(velocity, window_size, var_thresh))[0]

    dropout_candidates = np.union1d(np.where(low_var_mask)[0], flat_x)
    dropout_candidates = np.union1d(dropout_candidates, flat_z)
    dropout_candidates = np.union1d(dropout_candidates, flat_velocity)
    dropout_candidates = np.union1d(dropout_candidates, low_var_velocity_indices)

    to_exclude = sitting_mask | fog_mask
    safe_indices = np.where(~to_exclude)[0]
    final_dropout_indices = np.intersect1d(dropout_candidates, safe_indices)

    dropout_mask = np.zeros(len(data), dtype=bool)
    final_dropout_indices = final_dropout_indices[final_dropout_indices < len(data)]
    dropout_mask[final_dropout_indices] = True
    dropout_mask = binary_dilation(dropout_mask, structure=np.ones(2 * pad + 1))

    if debug:
        print("🧠 FOG-aware Dropout Cleaner Debug:")
        print(f" - Low variance cam_x & cam_z: {np.sum(low_var_mask)}")
        print(f" - Flat cam_x: {len(flat_x)}, Flat cam_z: {len(flat_z)}")
        print(f" - Flat velocity: {len(flat_velocity)}")
        print(f" - Low var velocity: {len(low_var_velocity_indices)}")
        print(f" - Sitting frames ignored: {np.sum(sitting_mask)}")
        print(f" - FOG frames protected: {np.sum(fog_mask)}")
        print(f" → Final dropout frames (before dilation): {len(final_dropout_indices)}")
        print(f" → Final dropout frames (after dilation): {np.sum(dropout_mask)}")

    keep_mask = ~dropout_mask
    return data[keep_mask], labels_main[keep_mask], labels_fog[keep_mask]


def fuzzy_match(pred_labels, true_labels, class_id, tolerance=5):
    matched = 0
    gt_indices = np.where(true_labels == class_id)[0]
    pred_indices = np.where(pred_labels == class_id)[0]
    pred_set = set(pred_indices)

    for idx in gt_indices:
        window = range(max(0, idx - tolerance), min(len(true_labels), idx + tolerance + 1))
        if any(i in pred_set for i in window):
            matched += 1

    return matched, len(gt_indices), matched / len(gt_indices) if len(gt_indices) > 0 else 0.0


    
def process_folder_of_files(
    folder_path,
    spike_threshold_factor=30,
    generate_plots=False,
    adjust_outliers=True,
    seated_height=1.125,
    standing_height=1.516,
    walking_velocity=0.609,
    height_offset=0.0,
    velocity_scale=1.0,
    tag="train",  #
    normalize = True
):


    """
    Process and load .mat/.hdf5 data from a folder. Handles outlier adjustments, computes statistics, and optionally generates plots.
    Focuses on generating input data, labels_main, and labels_prefog.
    """
    all_input_data = []
    all_labels_main = []
    all_labels_prefog = []
    seated_heights_all_files = []  # Initialize for seated heights
    standing_heights_all_files = []  # Initialize for standing heights
    walking_velocities_all_files = []  # ✅ This was missing
    all_files = []  # Collect metadata for all files
    ensemble_stats = {}
    skipped_files = []
    frame_counts = []
    file_names = []  # List to hold filenames
    label_summary_stats = {}  # To track total label frame counts across files

    label_summary_stats = {
        "Turning_1L": [],
        "Turning_1R": [],
        "Walking": [],
        "Turning_2L": [],
        "Turning_2R": [],
        "Gait_Initialization": [],
        "Gait_Termination": [],
        "FOG": [],
        "predFOG": [],
        "Standing": []
    }


    summary_data = {
        "Velocity_XZ": [],
        "Height_Above_Ground": [],
        "CamCorrected_X": [],
        "CamCorrected_Z": [],
        "CamCorrectedAngle": [],
        "Pitch": [],
        'ht_scaled': [],  # ✅ Add this explicitly
    }
    
    # Initial state analysis (majority voting approach)
    initial_state_summary = {
        "total_files": 0,
        "sitting_first": 0,
        "standing_first": 0,
        "seated_heights": [],  # Collect heights when sitting
        "standing_heights": []  # Collect heights when standing
    }

    file_statistics = {feature: [] for feature in summary_data.keys()}  # To store file-level stats

    # Load .hdf5 files
    hdf5_files = [f for f in os.listdir(folder_path) if f.endswith('.hdf5')]
    if len(hdf5_files) == 0:
        print(f"No .hdf5 files found in folder: {folder_path}")
        return None, None, None, None, None, None, None, None


    print(f"Found {len(hdf5_files)} files in folder: {folder_path}")

    # Process all files
    for filename in hdf5_files:
        hdf5_file_path = os.path.join(folder_path, filename)
        with h5py.File(hdf5_file_path, 'r') as hdf5_file:
            print(f"\nProcessing file: {filename}")

            # ✅ STEP 1: Extract `height_above_ground`
            height_above_ground = np.copy(hdf5_file['camCorrected_unfiltered'][:, 1])
            print(f"✅ Initial height_above_ground shape: {height_above_ground.shape}")
            print(f"🔍 Min/Max BEFORE floor_y correction: {np.min(height_above_ground):.6f} - {np.max(height_above_ground):.6f}")

            # ✅ STEP 2: Extract `floor_y` (or use fallback)
            if 'floory_Global' in hdf5_file:
                floor_y = np.squeeze(np.copy(hdf5_file['floory_Global'][:]))   # Read from file
                print(f"✅ Extracted 'floory_Global' shape: {floor_y.shape}")
                print(f"✅ First 5 floor_y values: {floor_y[:5].flatten()}")
                print(f"✅ Min/Max floor_y: {np.min(floor_y):.6f} - {np.max(floor_y):.6f}")

                # If `floor_y` is a single value, broadcast it
                if floor_y.size == 1:
                    floor_y = np.full_like(height_above_ground, floor_y.item())  
                    print(f"✅ Broadcasted 'floor_y' to shape {floor_y.shape}")

                # Ensure shape consistency
                elif floor_y.shape != height_above_ground.shape:
                    print(f"⚠️ Warning: Shape mismatch! Resizing 'floor_y' from {floor_y.shape} to {height_above_ground.shape}")
                    floor_y = np.resize(floor_y, height_above_ground.shape)  

            else:
                floor_y = np.full_like(height_above_ground, -1.6)  # Fallback value
                print("⚠️ Warning: No 'floor_y_global' found. Using default -1.6")

            # ✅ STEP 3: Restore height_above_ground to original scale
            height_above_ground -= floor_y

            # ✅ Initialize feature dictionary
            processed_features = {}

            # ✅ Normalize or keep raw height
            if normalize:
                ht_scaled = (height_above_ground - seated_height) / (standing_height - seated_height + 1e-6)
                ht_scaled = np.clip(ht_scaled, 0.0, 1.2)
            else:
                ht_scaled = height_above_ground

            # ✅ Store normalized or raw height
            processed_features["ht_scaled"] = ht_scaled


            # ✅ Debugging Output
            print(f"✅ Min/Max height_above_ground AFTER floor_y correction: {np.min(height_above_ground):.6f} - {np.max(height_above_ground):.6f}")
            print(f"✅ First 5 values of corrected height: {height_above_ground[:5].flatten()}")
            print(f"✅ First 5 values of ht_scaled: {ht_scaled[:5].flatten()}")

            # Load and standardize features
            raw_features = {
                "Velocity_XZ": np.copy(hdf5_file['VelocityXZ_unfiltered'][:]),
                "Height_Above_Ground": height_above_ground,
                "CamCorrected_X": np.copy(hdf5_file['camCorrected_unfiltered'][:, 0]),
                "CamCorrected_Z": np.copy(hdf5_file['camCorrected_unfiltered'][:, 2]),
                "CamCorrectedAngle": np.copy(hdf5_file['camCorrected'][:, 4]),  # yaw
                "Pitch": np.copy(hdf5_file['camCorrected'][:, 3]),              # pitch
                "ht_scaled": ht_scaled                                          # ✅ Added here
            }

            # Ensure all features have consistent shape (x, 1)
            for feature_name, feature_data in raw_features.items():
                if len(feature_data.shape) == 1:  # Convert 1D arrays to 2D
                    raw_features[feature_name] = feature_data[:, np.newaxis]
                    print(f"Debug: Converted {feature_name} to shape {raw_features[feature_name].shape}")

            # Ensure all features have consistent shape (x, 1)
            for feature_name, feature_data in raw_features.items():
                if len(feature_data.shape) == 1:  # Convert 1D arrays to 2D
                    raw_features[feature_name] = feature_data[:, np.newaxis]
                    print(f"Debug: Converted {feature_name} to shape {raw_features[feature_name].shape}")

            # ✅ Extract initial turn labels and FOG
            turnings_l = hdf5_file['Turning1L'][:].astype(int)
            turnings_r = hdf5_file['Turning1R'][:].astype(int)
            turnings_2l = hdf5_file['Turning2L'][:].astype(int)
            turnings_2r = hdf5_file['Turning2R'][:].astype(int)
            walkings = hdf5_file['Walkings'][:].astype(int)
            gait_inits = hdf5_file['GaitInitiations'][:].astype(int)  # Separate out gait initiation
            FOGs = np.array(hdf5_file['FOGs']).astype(int)  # Convert to NumPy array for consistency
            predfogs = hdf5_file['predictionFOG'][:] if 'predictionFOG' in hdf5_file else np.zeros_like(fogs)
            predfogs_binary = (predfogs >= 1).astype(int)
            standings = hdf5_file['Standings'][:].astype(int) 
            gait_terms = hdf5_file['GaitTerminations'][:].astype(int)
    
            fog_indices = np.where(FOGs == 1)[0] # Should be 0
        
            if np.sum(FOGs) > 0:
                print(f"✅ File {filename} contains {np.sum(FOGs)} FOG frames.")
            else:
                print(f"❌ File {filename} contains NO FOG frames.")


            # ✅ Stack labels into final array (excluding Gait_Init)
            real_time_labels_main = np.column_stack([
                hdf5_file['Sittings'][:], standings,
                hdf5_file['SitToStands'][:], hdf5_file['StandToSits'][:], 
                turnings_l, turnings_r, turnings_2l, turnings_2r, walkings, gait_inits, gait_terms, FOGs, predfogs
            ])


            print(f"\n[Initial] Standing sum: {np.sum(real_time_labels_main[:, 1])}")
            print(f"[Initial] Walking sum:  {np.sum(real_time_labels_main[:, 8])}")
            print(f"[Initial] Shape:        {real_time_labels_main.shape}")
            row_sums = np.sum(real_time_labels_main, axis=1)
            unique_row_sums = np.unique(row_sums)
            #print("🧪 Unique row sums:", unique_row_sums)

            
            # --- Track per-file label frame counts ---
            if 'label_summary_stats' not in locals():
                label_summary_stats = {
                    "Turning_1L": [],
                    "Turning_1R": [],
                    "Walking": [],
                    "Turning_2L": [],
                    "Turning_2R": [],
                    "Gait_Initialization": [],
                    "Gait_Termination": [],
                    "FOG": [],
                    "predFOG": [],
                    "Standing": []
                }

            label_summary_stats["Turning_1L"].append(np.sum(turnings_l))
            label_summary_stats["Turning_1R"].append(np.sum(turnings_r))
            label_summary_stats["Walking"].append(np.sum(walkings))
            label_summary_stats["Turning_2L"].append(np.sum(turnings_2l))
            label_summary_stats["Turning_2R"].append(np.sum(turnings_2r))
            label_summary_stats["Gait_Initialization"].append(np.sum(gait_inits))
            label_summary_stats["Gait_Termination"].append(np.sum(gait_terms))
            label_summary_stats["FOG"].append(np.sum(FOGs))
            label_summary_stats["predFOG"].append(np.sum(predfogs))
            label_summary_stats["Standing"].append(np.sum(standings))
 

            # ✅ Extract Walking, Standing, and Sitting Labels
            walkings = real_time_labels_main[:, 8]  # Walking label
            standings = real_time_labels_main[:, 1]  # Standing label
            sittings = real_time_labels_main[:, 0]  # Sitting label
            FOGS_label = real_time_labels_main[:, 11]  # FOGS label
            pure_standing_mask = (standings == 1) & (FOGS_label == 0)

            # ✅ Compute Normalization References (Raw Values Before Normalization)
            walking_indices = np.where(walkings == 1)[0]
#             standing_indices = np.where(standings == 1)[0]
            standing_indices = np.where(pure_standing_mask)[0]
            sitting_indices = np.where(sittings == 1)[0]
            
            # ✅ Compute raw (pre-normalization) heights & velocity
            raw_avg_walking_velocity = (
                np.mean(raw_features["Velocity_XZ"][walking_indices])
                if len(walking_indices) > 0 and np.any(raw_features["Velocity_XZ"][walking_indices])
                else np.mean(raw_features["Velocity_XZ"])
            )
            raw_avg_walking_velocity = max(raw_avg_walking_velocity, 1e-8)  # Ensure nonzero

            # Ensure there are valid standing indices before extracting heights
            if len(standing_indices) > 0 and np.any(height_above_ground[standing_indices]):
                sorted_standing_heights = np.sort(height_above_ground[standing_indices])
                n_standing = len(sorted_standing_heights)

                # Define middle 25% range for standing heights
                lower_idx_standing = int(n_standing * 0.375)  # 37.5th percentile
                upper_idx_standing = int(n_standing * 0.625)  # 62.5th percentile

                # Extract stable middle 25% and compute median
                stable_standing_subset = sorted_standing_heights[lower_idx_standing:upper_idx_standing]
                raw_avg_standing_height = (
                    np.median(stable_standing_subset)
                    if len(stable_standing_subset) > 0
                    else np.mean(sorted_standing_heights)
                )
            else:
                raw_avg_standing_height = np.mean(height_above_ground)

            # Prevent division by zero
            raw_avg_standing_height = max(raw_avg_standing_height, 1e-8)


            # Ensure there are valid sitting indices before extracting heights
            if len(sitting_indices) > 0 and np.any(height_above_ground[sitting_indices]):
                sorted_sitting_heights = np.sort(height_above_ground[sitting_indices])
                n_sitting = len(sorted_sitting_heights)

                # Define middle 25% range for sitting heights
                lower_idx_sitting = int(n_sitting * 0.375)  # 37.5th percentile
                upper_idx_sitting = int(n_sitting * 0.625)  # 62.5th percentile

                # Extract stable middle 25% and compute median
                stable_sitting_subset = sorted_sitting_heights[lower_idx_sitting:upper_idx_sitting]
                raw_avg_seated_height = (
                    np.median(stable_sitting_subset)
                    if len(stable_sitting_subset) > 0
                    else np.mean(sorted_sitting_heights)
                )
            else:
                raw_avg_seated_height = np.mean(height_above_ground)

            # Prevent division by zero
            raw_avg_seated_height = max(raw_avg_seated_height, 1e-8)

            
            # ✅ Print Debug Info for Each File
            print(f"\n🔍 **Debugging Heights for File: {filename}**")
            print(f"  - Walking Velocity (Raw): {raw_avg_walking_velocity:.6f}")
            print(f"  - Standing Height (Raw): {raw_avg_standing_height:.6f}")
            print(f"  - Seated Height (Raw): {raw_avg_seated_height:.6f}")

            ### **⏬ ROW SUM ANALYSIS & MISSING LABEL CHECK ⏬**

            ## ✅ Compute total active labels per timestep
            row_sums = np.sum(real_time_labels_main, axis=1)

            # ⛳️ Debug #1: Raw height range BEFORE normalization
#             print(f"\n[DEBUG] ⬇️ Raw height_above_ground stats BEFORE normalization")
#             print(f"Min raw height_above_ground: {np.min(height_above_ground):.5f}")
#             print(f"Max raw height_above_ground: {np.max(height_above_ground):.5f}")
#             print(f"First 5 height_above_ground values: {height_above_ground[:5].flatten()}")

            # ⛳️ Debug #2: Height stats by LABEL
            print("\n[DEBUG] ⬇️ Seated / Standing height values from label ranges")

            if len(sitting_indices) > 0:
                seated_values = height_above_ground[sitting_indices]
                print(f"Sitting values → Min: {np.min(seated_values):.5f}, Max: {np.max(seated_values):.5f}, Median: {np.median(seated_values):.5f}")
            else:
                print("⚠️ No sitting frames found!")

            if len(standing_indices) > 0:
                standing_values = height_above_ground[standing_indices]
                print(f"Standing values → Min: {np.min(standing_values):.5f}, Max: {np.max(standing_values):.5f}, Median: {np.median(standing_values):.5f}")
            else:
                print("⚠️ No standing frames found!")

            # ⛳️ Debug #3: Label counts summary
            label_counts = np.sum(real_time_labels_main, axis=0)
            print("\n[DEBUG] ⬇️ Label distribution for current file:")
            for i, count in enumerate(label_counts):
                print(f"Label {i} count: {count}")

                
            if seated_height < 0.5 or standing_height > 5.0:
                # first pass – use per-file raw heights
                pass
            else:
                # second pass – use global fixed values
                print("🔒 Using global fixed heights — skipping per-file recalculations.")
                raw_avg_seated_height = seated_height
                raw_avg_standing_height = standing_height

            print(f"\n🔍 DEBUG: height_offset = {height_offset:.5f}")
            print(f"🔍 DEBUG: seated_height = {seated_height:.5f}, standing_height = {standing_height:.5f}")
            
            print(f"\n🔍 DEBUG: Using seated_height = {raw_avg_seated_height:.5f}, standing_height = {raw_avg_standing_height:.5f}")
            assert raw_avg_seated_height > 0.2, "❌ Seated height too low — normalization input is invalid!"
            assert raw_avg_standing_height > 0.5, "❌ Standing height too low — normalization input is invalid!"


            # ✅ Compute raw height range
            raw_height_range = raw_avg_standing_height - raw_avg_seated_height

            print(f"📉 Raw Seated Height: {seated_height:.5f}")
            print(f"📉 Raw Standing Height: {standing_height:.5f}")
            print(f"📐 Computed Raw Height Range: {raw_height_range:.5f}")

            # ✅ Normalize height: seated → 0.0, standing → 1.0
            normalized_height_raw = (height_above_ground - raw_avg_seated_height) / raw_height_range

            print(f"⚠️ Pre-clip → Min: {normalized_height_raw.min():.3f}, Max: {normalized_height_raw.max():.3f}")
            height_above_ground = np.clip(normalized_height_raw, 0, 1)
            print(f"✅ Height normalized → Min: {height_above_ground.min():.3f}, Max: {height_above_ground.max():.3f}")


            # ✅ Extract label indices
            walking_indices = np.where(real_time_labels_main[:, 8] == 1)[0]
            standing_indices = np.where(real_time_labels_main[:, 1] == 1)[0]

            # ✅ Extract raw velocity
            raw_velocity = raw_features["Velocity_XZ"].copy()

            # ✅ Compute medians
            if len(standing_indices) > 0 and np.any(raw_velocity[standing_indices]):
                v_still = 0 #np.median(raw_velocity[standing_indices])
            else:
                v_still = 0 #np.percentile(raw_velocity, 10)  # fallback estimate for near-zero movement

            if len(walking_indices) > 0 and np.any(raw_velocity[walking_indices]):
                v_walk = np.median(raw_velocity[walking_indices])
            else:
                v_walk = np.percentile(raw_velocity, 90)  # fallback for upper dynamic range

            # ✅ Prevent divide-by-zero
            v_range = max(v_walk - v_still, 1e-6)

            # ✅ Dual-anchor normalization: standing = 0, walking = 1
            normalized_velocity_xz = (raw_velocity - v_still) / v_range

            # ✅ Recompute medians
            final_seated_height = np.median(height_above_ground[sitting_indices]) if len(sitting_indices) > 0 else 0.0
            final_standing_height = np.median(height_above_ground[standing_indices]) if len(standing_indices) > 0 else 1.0

            final_walking_velocity = np.median(normalized_velocity_xz[walking_indices]) if len(walking_indices) > 0 else 0.0

            print(f"✅ Recomputed normalized seated height median: {final_seated_height:.5f}")
            print(f"✅ Recomputed normalized standing height median: {final_standing_height:.5f}")
            print(f"✅ Recomputed normalized walking velocity median: {final_walking_velocity:.5f}")
            
            print(f"✅ velocity still raw: {v_still:.5f}")
            print(f"✅ velocity walking raw: {v_walk:.5f}")

            # ✅ Append results
            seated_heights_all_files.append(final_seated_height)
            standing_heights_all_files.append(final_standing_height)
            walking_velocities_all_files.append(final_walking_velocity)

            # ✅ Handle NaNs
            valid_seated_heights = [h for h in seated_heights_all_files if not np.isnan(h)]
            valid_standing_heights = [h for h in standing_heights_all_files if not np.isnan(h)]
            valid_walking_velocities = [v for v in walking_velocities_all_files if not np.isnan(v)]

            default_seated_height = np.mean(valid_seated_heights) if valid_seated_heights else 0.0
            default_standing_height = np.mean(valid_standing_heights) if valid_standing_heights else 1.0
            default_walking_velocity = np.mean(valid_walking_velocities) if valid_walking_velocities else 0.0

            for i, (seated, standing, walking) in enumerate(zip(seated_heights_all_files, standing_heights_all_files, walking_velocities_all_files)):
                if np.isnan(seated):
                    seated_heights_all_files[i] = default_seated_height
                if np.isnan(standing):
                    standing_heights_all_files[i] = default_standing_height
                if np.isnan(walking):
                    walking_velocities_all_files[i] = default_walking_velocity

            # ✅ Clean and process features
            processed_features = {}
            for feature_name, feature_data in raw_features.items():
                spike_threshold = np.mean(feature_data) + 3 * np.std(feature_data)
                spikes = np.where(feature_data > spike_threshold)[0]
                feature_data[spikes] = spike_threshold
                processed_features[feature_name] = feature_data
                summary_data[feature_name].append(feature_data)

            # ✅ Final normalization using robust velocity
            processed_features["Height_Above_Ground"] = height_above_ground
            processed_features["Velocity_XZ"] = normalized_velocity_xz
            
            # 🧪 ASSERT feature alignment and label alignment
            num_timesteps = next(iter(processed_features.values())).shape[0]
            assert all(v.shape[0] == num_timesteps for v in processed_features.values()), \
                "❌ Processed features not aligned across dimensions!"

            assert real_time_labels_main.shape[0] == num_timesteps, \
                "❌ Labels not aligned with processed features!"

            # ✅ Combine into input array
            input_data = np.column_stack([
                processed_features["Velocity_XZ"],
                processed_features["Height_Above_Ground"],
                processed_features["CamCorrected_X"],
                processed_features["CamCorrected_Z"],
                processed_features["CamCorrectedAngle"],
                processed_features["Pitch"]
            ])

            print(f"\n[Final] Standing sum: {np.sum(real_time_labels_main[:, 1])}")
            print(f"[Final] Walking sum:  {np.sum(real_time_labels_main[:, 8])}")
            print(f"[Final] Shape:        {real_time_labels_main.shape}")
            row_sums = np.sum(real_time_labels_main, axis=1)
            unique_row_sums = np.unique(row_sums)
            #print("🧪 Unique row sums:", unique_row_sums)
    
            
            print(f"SPECIAL TEST ---> Seated height: {raw_avg_seated_height}")
            print(f"Standing height: {raw_avg_standing_height}")
            print(f"Raw height range: {raw_avg_standing_height - raw_avg_seated_height}")

            fog_indices = np.where(FOGs == 1)[0]
            print(f"# FOG frames: {len(fog_indices)}")

            # Raw heights during FOG
#             print("Raw heights during FOG:", (normalized_height_raw * (raw_avg_standing_height - raw_avg_seated_height) + raw_avg_seated_height)[fog_indices])
#             print("Normalized heights during FOG:", normalized_height_raw[fog_indices])
#             print("Clipped heights during FOG:", height_above_ground[fog_indices])

            # Store processed data and labels
            all_input_data.append(input_data)
            all_labels_main.append(real_time_labels_main)
            all_labels_prefog.append(predfogs_binary)
            file_names.append(filename)
            print(f"Appended data from {filename}: Input Data Shape = {input_data.shape}, Labels Shape = {real_time_labels_main.shape}")
            print(f"✅ Successfully added {filename} to processed list.")
    
    print(f"📏 Median Seated Height: {final_seated_height:.3f}")
    print(f"📏 Median Standing Height: {final_standing_height:.3f}")
    print(f"🚮 Median Walking Velocity: {final_walking_velocity:.3f}")

    print(f"\n🔍 **Standing Count Across All Files:** {sum(standing_heights_all_files)}")

    print(f"🔍 File processing complete. Total files processed: {len(file_names)}")
    
    
    if 'label_summary_stats' in locals():
        print("\n📊 Summary of Label Counts Across All Files:")
        for label_name, counts in label_summary_stats.items():
            print(f"\n🔹 {label_name}")
            print(f"   Total Frames: {np.sum(counts)}")
            print(f"   Mean:         {np.mean(counts):.2f}")
            print(f"   Median:       {np.median(counts):.2f}")
            print(f"   Min/Max:      {np.min(counts)} / {np.max(counts)}")
            print(f"   Std Dev:      {np.std(counts):.2f}")
                  
    # Concatenate all data and labels after processing
    all_input_data = np.concatenate(all_input_data, axis=0) if all_input_data else None
    all_labels_main = np.concatenate(all_labels_main, axis=0) if all_labels_main else None
    all_labels_prefog = np.concatenate(all_labels_prefog, axis=0) if all_labels_prefog else None
    
    if all_input_data is not None and all_input_data.size > 0:
        print(f"Shape of concatenated all_input_data: {all_input_data.shape}")
    else:
        print("No input data after concatenation.")
    
    if all_labels_main is not None:
        print(f"Shape of concatenated all_labels_main: {all_labels_main.shape}")
    else:
        print("No labels_main data after concatenation.")

    # Generate ensemble statistics and identify outliers
    for feature_name, all_feature_data in summary_data.items():
        all_data = np.concatenate(all_feature_data, axis=0)

        # Compute statistics
        ensemble_mean = np.mean(all_data)
        ensemble_std = np.std(all_data)
        ensemble_range = np.max(all_data) - np.min(all_data)
        ensemble_median = np.median(all_data)
        ensemble_min = np.min(all_data)
        ensemble_max = np.max(all_data)
        # Ensure skewness and kurtosis are scalars (not arrays)
        ensemble_skewness = float(skew(all_data, nan_policy='omit'))
        ensemble_kurtosis = float(kurtosis(all_data, nan_policy='omit'))


        print(f"Calculating ensemble stats for {feature_name}...")
                         
        # Compute median and MAD
        ensemble_median = np.median(all_data)
        mad = np.median(np.abs(all_data - ensemble_median))

        # Compute modified Z-score for each file's mean
        outlier_files = [
            stats["filename"] for stats in file_statistics[feature_name]
            if mad > 0 and (0.6745 * (stats["mean"] - ensemble_median) / mad) > 3.5
    ]
        # Store computed statistics
        ensemble_stats[feature_name] = {
            "mean": ensemble_mean,
            "std": ensemble_std,
            "range": ensemble_range,
            "median": ensemble_median,
            "min": ensemble_min,
            "max": ensemble_max,
            "skewness": ensemble_skewness,
            "kurtosis": ensemble_kurtosis,
            "outliers": outlier_files
        }

        print(f"\nFeature: {feature_name}")
        print(f"Mean: {ensemble_mean:.4f}, Std Dev: {ensemble_std:.4f}, Range: {ensemble_range:.4f}")
        print(f"Median: {ensemble_median:.4f}, Min: {ensemble_min:.4f}, Max: {ensemble_max:.4f}")
        print(f"Skewness: {ensemble_skewness:.4f}, Kurtosis: {ensemble_kurtosis:.4f}")
        print(f"Outliers: {len(outlier_files)} files: {outlier_files}")

        if generate_plots:
            print(f"Generating ensemble summary plot for {feature_name}")
            plt.figure(figsize=(12, 6))
            for feature_data in all_feature_data:
                plt.plot(feature_data, alpha=0.4)
            plt.axhline(ensemble_mean, color="r", linestyle="--", label="Ensemble Mean")
            plt.axhline(ensemble_mean + 3 * ensemble_std, color="g", linestyle="--", label="Mean ± 3*Std Dev")
            plt.axhline(ensemble_mean - 3 * ensemble_std, color="g", linestyle="--")
            plt.title(f"Summary Plot for {feature_name} Across All Files")
            plt.xlabel("Index")
            plt.ylabel(feature_name)
            plt.legend(loc="upper right")
            plt.show()

    # Concatenate and finalize summary data
    for feature_name, feature_data_list in summary_data.items():
        summary_data[feature_name] = np.concatenate(feature_data_list, axis=0)
        print(f"Final concatenated data for {feature_name}: {summary_data[feature_name].shape}")

#     # Print all seated and standing heights collected from raw files
#     print("\n=== Raw Seated Heights Collected ===")
#     for i, height in enumerate(seated_heights_all_files):
#         if height is not None:
#             print(f"File {i + 1}: Seated Height = {height:.2f}")

#     print("\n=== Raw Standing Heights Collected ===")
#     for i, height in enumerate(standing_heights_all_files):
#         if height is not None:
#             print(f"File {i + 1}: Standing Height = {height:.2f}")

    # Final reporting
    # Filter out None values
    valid_seated_heights = [height for height in seated_heights_all_files if height is not None]
    valid_standing_heights = [height for height in standing_heights_all_files if height is not None]

    # Convert to numpy arrays
    seated_heights = np.array(valid_seated_heights)
    standing_heights = np.array(valid_standing_heights)

    # Print statistics
    if len(seated_heights) > 0:
        print(f"\n=== Seated Height Summary ===")
        print(f"Average: {np.mean(seated_heights):.2f} ± {np.std(seated_heights):.2f} (n={len(seated_heights)})")
    else:
        print("No valid seated heights available.")

    if len(standing_heights) > 0:
        print(f"\n=== Standing Height Summary ===")
        print(f"Average: {np.mean(standing_heights):.2f} ± {np.std(standing_heights):.2f} (n={len(standing_heights)})")
    else:
        print("No valid standing heights available.")

    # After the loop, summarize skipped files
    print(f"\nSkipped {len(skipped_files)} files due to insufficient frames: {skipped_files}")

    print(f"\nShape of all_labels_main after creation: {all_labels_main.shape if all_labels_main is not None else 'None'}")

    ###########temp################
    
    
    def check_raw_gt_durations(gt_labels):
        """
        Computes the longest continuous STS and STSit durations from the original GT labels.
        """
        # Extract STS and STSit labels from raw GT
        labeled_sts = gt_labels[:, 2]  # Class 2 (STS)
        labeled_stsit = gt_labels[:, 3]  # Class 3 (STSit)

        # Identify continuous STS segments
        sts_labels, num_sts = label(labeled_sts)
        stsit_labels, num_stsit = label(labeled_stsit)

        # Compute the longest STS duration
        max_sts_duration = max([np.sum(sts_labels == i) for i in range(1, num_sts + 1)] or [0])

        # Compute the longest STSit duration
        max_stsit_duration = max([np.sum(stsit_labels == i) for i in range(1, num_stsit + 1)] or [0])

        # Print the results
        print(f"🔍 Longest **RAW GT** STS Duration: {max_sts_duration} frames")
        print(f"🔍 Longest **RAW GT** STSit Duration: {max_stsit_duration} frames")

    
    check_raw_gt_durations(all_labels_main)
    
    ###############
    print("🧪 RETURNING from process_folder_of_files:")
    print("  - input_data:", type(all_input_data))
    print("  - labels_main:", type(all_labels_main))
    print("  - labels_prefog:", type(all_labels_prefog))
    print("  - summary_data:", type(summary_data))
    print("  - ensemble_stats:", type(ensemble_stats))
    print("  - seated_heights:", len(seated_heights_all_files))
    print("  - standing_heights:", len(standing_heights_all_files))
    print("  - file_names:", len(file_names))
    
    assert all_input_data.shape[0] == all_labels_main.shape[0] == all_labels_prefog.shape[0], \
    "❌ Final output alignment issue!"

    return (
        all_input_data,
        all_labels_main,
        all_labels_prefog,
        summary_data,
        ensemble_stats,
        seated_heights_all_files,
        standing_heights_all_files,
        file_names
    )


def prepare_data(train_folder, val_folder, class_mapping, generate_plots=False):
    """
    Prepare train and validation data from respective folders using process_folder_of_files.
    Applies a two-pass system for both train and validation: first for gathering cleaned medians, second for applying normalization.
    """

    def process_and_reshape(
        folder_path,
        generate_plots,
        tag="train",
        seated_height=None,
        standing_height=None,
        walking_velocity=None,
        height_offset=0.0
    ):
        print(f"\n🚚 Processing folder: {folder_path} ({tag.upper()})")

        # ✅ Only normalize heights if valid values are passed
        normalize_heights = seated_height is not None and standing_height is not None

        (
            all_input_data,
            all_labels_main,
            all_labels_prefog,
            summary_data,
            ensemble_stats,
            seated_heights,
            standing_heights,
            file_names,
        ) = process_folder_of_files(
            folder_path=folder_path,
            generate_plots=generate_plots,
            seated_height=seated_height,
            standing_height=standing_height,
            walking_velocity=walking_velocity or 0.609,
            height_offset=height_offset,
            normalize=normalize_heights,  # ✅ Key fix: pass flag into downstream logic
            tag=tag
        )

        if len(all_input_data) == 0 or len(all_labels_main) == 0:
            raise ValueError(f"🚨 No data loaded for {tag.upper()} — check your input files.")

        # Fill bad seated/standing height values if needed
        seated_heights = [h if h > 0 else seated_height or 0.0 for h in seated_heights]
        standing_heights = [h if h > 0 else standing_height or 1.0 for h in standing_heights]

        frames_per_file = len(all_input_data) // len(seated_heights)
        expanded_seated = np.repeat(seated_heights, frames_per_file)
        expanded_standing = np.repeat(standing_heights, frames_per_file)

        # Padding if uneven file splits
        if len(expanded_seated) < len(all_input_data):
            expanded_seated = np.append(expanded_seated, [seated_heights[-1]] * (len(all_input_data) - len(expanded_seated)))
        if len(expanded_standing) < len(all_input_data):
            expanded_standing = np.append(expanded_standing, [standing_heights[-1]] * (len(all_input_data) - len(expanded_standing)))

        cleaned_data, cleaned_main, cleaned_prefog = clean_dropout_frames(
            data=all_input_data,
            labels_main=all_labels_main,
            labels_fog=all_labels_prefog,
            cam_x_idx=2,
            cam_z_idx=3,
            velocity_idx=0,
            pad=1,
            var_thresh=1e-9,
            window_size=7,
            min_flat_len=5,
            velocity_thresh=0.002,
            velocity_flat_len=12,
            sitting_class_index=0,
            fog_column_index=0,
            debug=True
        )

        assert (
            cleaned_data.shape[0] == cleaned_main.shape[0] == cleaned_prefog.shape[0]
        ), f"❌ Dropout filter misaligned data ({cleaned_data.shape[0]} vs {cleaned_main.shape[0]} vs {cleaned_prefog.shape[0]})"

        if cleaned_data.size == 0:
            raise ValueError(f"🚨 All {tag.upper()} data was removed during dropout filtering.")

        return (
            cleaned_data,
            cleaned_main,
            cleaned_prefog,
            ensemble_stats,
            summary_data,
            expanded_seated,
            expanded_standing,
            file_names
        )


    print("🔄 Starting preprocessing pipeline...")

    # === First Pass on Training Data ===
    print("\n🔎 [PASS 1] Gathering medians from cleaned training data...")
    cleaned_data_train_rawpass, cleaned_main_train_rawpass, _, _, _, _, _, _ = process_and_reshape(
        folder_path=train_folder,
        generate_plots=False,
        tag="rawpass",
        seated_height=0.0,
        standing_height=100.0,
        height_offset=0.0
    )

    frame_labels_train = np.argmax(cleaned_main_train_rawpass, axis=-1)
    HEIGHT_IDX = 1
    seated_vals_train = cleaned_data_train_rawpass[frame_labels_train == 0, HEIGHT_IDX]
    standing_vals_train = cleaned_data_train_rawpass[frame_labels_train == 1, HEIGHT_IDX]

    median_seated = np.median(seated_vals_train[seated_vals_train > 0]) if len(seated_vals_train) > 0 else 0.0
    median_standing = np.median(standing_vals_train[standing_vals_train > 0]) if len(standing_vals_train) > 0 else 1.0
    height_offset = 1.0 - median_standing

    print(f"\n📊 Cleaned Medians (Train):")
    print(f"   Seated Height     → {median_seated:.5f}")
    print(f"   Standing Height   → {median_standing:.5f}")
    print(f"🔧 Height Offset      = {height_offset:.5f}")

    # === Second Pass on Training Data ===
    train_data, train_main, train_prefog, train_stats, train_summary, train_seated_heights, train_standing_heights, _ = process_and_reshape(
        folder_path=train_folder,
        generate_plots=generate_plots,
        tag="train",
        seated_height=median_seated,
        standing_height=median_standing,
        height_offset=height_offset
    )

    # === Validation Pass (Using Training Medians) ===
    print("\n🔎 [VAL PASS] Using training medians for validation normalization...")
    val_data, val_main, val_prefog, val_stats, val_summary, val_seated_heights, val_standing_heights, _ = process_and_reshape(
        folder_path=val_folder,
        generate_plots=generate_plots,
        tag="val",
        seated_height=median_seated,         # ✅ Reuse TRAIN values
        standing_height=median_standing,     # ✅ Reuse TRAIN values
        height_offset=height_offset          # ✅ Reuse TRAIN offset
    )

    print("\n📅 Final Median Checks Across Datasets:")
    print(f"📏 Seated Height  — Train: {np.median(train_seated_heights):.5f} | Val: {np.median(val_seated_heights):.5f}")
    print(f"📏 Standing Height — Train: {np.median(train_standing_heights):.5f} | Val: {np.median(val_standing_heights):.5f}")

    return (
        train_data, train_main, train_prefog,
        val_data, val_main, val_prefog,
        train_stats, val_stats,
        train_summary, val_summary,
        train_seated_heights, val_seated_heights,
        train_standing_heights, val_standing_heights
    )


def validate_shapes(X, y):
    """
    Validate that data and labels are aligned in shape and sequence length.
    
    Args:
        X (numpy array): Input data.
        y (numpy array): Corresponding labels.
    """
    assert X.shape[0] == y.shape[0], f"Mismatch in sequence count! X: {X.shape[0]}, y: {y.shape[0]}"
    assert X.shape[1] == y.shape[1], f"Mismatch in sequence length! X: {X.shape[1]}, y: {y.shape[1]}"
    print("✅ Data and labels are properly aligned.")    
    
    
###############################################################################
#                          Data Augmentation Functions                        #
###############################################################################


def magnitude_scale(sequence, scale_range=(0.99, 1.01)):  # Expanded range for stronger augmentation .85/1.15
    """
    Scale the magnitude of a sequence by a random factor in `scale_range`.
    """
    scale = np.random.uniform(*scale_range)
    return sequence * scale

def add_noise(sequence, noise_level=0.001, feature_indices=None, use_std=True, dynamic=True):
    """
    Inject Gaussian noise into specific features of the sequence with adaptive scaling.
    """
    noisy_sequence = sequence.copy()

    # 1D (single feature)
    if noisy_sequence.ndim == 1:
        noise_scale = (
            np.std(noisy_sequence) * np.random.uniform(0.001, 0.02)  # Adaptive noise scaling
            if dynamic else (np.std(noisy_sequence) * 0.1 if use_std else noise_level)
        )
        noise = np.random.normal(0, noise_scale, size=noisy_sequence.shape)
        noisy_sequence += noise
        return noisy_sequence

    # 2D (multiple features)
    if feature_indices is None:
        feature_indices = range(noisy_sequence.shape[1])  # Apply to all features

    for feature_idx in feature_indices:
        noise_scale = (
            np.std(noisy_sequence[:, feature_idx]) * np.random.uniform(0.001, 0.02)  # Adaptive noise scaling
            if dynamic else (np.std(noisy_sequence[:, feature_idx]) * 0.1 if use_std else noise_level)
        )
        noise = np.random.normal(0, noise_scale, size=noisy_sequence[:, feature_idx].shape)
        noisy_sequence[:, feature_idx] += noise

    return noisy_sequence

def add_drift(sequence, drift_level=0.005, apply_to_features=None):
    """
    Introduce cumulative drift to simulate real-world sensor noise.
    """
    drifted_sequence = sequence.copy()

    if drifted_sequence.ndim == 1:
        drift = np.cumsum(np.random.normal(0, drift_level, size=drifted_sequence.shape))
        return drifted_sequence + drift

    if apply_to_features is None:
        apply_to_features = range(drifted_sequence.shape[1])  # Apply to all features by default

    for feature_idx in apply_to_features:
        drift = np.cumsum(np.random.normal(0, drift_level, size=drifted_sequence[:, feature_idx].shape))
        drifted_sequence[:, feature_idx] += drift

    return drifted_sequence


def adjust_oversampling(train_counts, val_counts, base_factors, max_factor=10, min_factor=2):
    """ Adjusts oversampling dynamically based on validation proportions while capping max limits. """
    adjusted_factors = {}

    # Compute total counts
    total_train = sum(train_counts.values())
    total_val = sum(val_counts.values())

    # Compute target scaling factor based on validation proportions
    for class_idx, train_count in train_counts.items():
        val_count = val_counts.get(class_idx, 1)  # Avoid division by zero
        train_ratio = train_count / total_train
        val_ratio = val_count / total_val

        # Compute required scaling to match validation ratio
        scaling_factor = val_ratio / train_ratio
        new_factor = int(round(base_factors[class_idx] * scaling_factor))

        # Apply caps
        adjusted_factors[class_idx] = max(min_factor, min(max_factor, new_factor))

    return adjusted_factors


def split_long_sequences(data, labels, max_lengths=None):
    if max_lengths is None:
        max_lengths = {  
            0: 500, 1: 500, 2: 300, 3: 300, 4: 600, 5: 600, 6: 600, 7: 600, 8: 800  
        }

    new_data, new_labels = [], []

    for i in range(len(data)):
        sequence = data[i]
        label_seq = labels[i]

        # 🔍 Determine the dominant class
        if label_seq.ndim == 1:
            dominant_class = np.argmax(np.bincount(label_seq.astype(int)))  
        else:
            dominant_class = np.argmax(np.bincount(np.argmax(label_seq, axis=-1).astype(int)))  

        max_segment_length = max_lengths.get(dominant_class, 500)

        # 🔥 DEBUG: Print pre-split details
        #print(f"🔍 Original Sequence {i}: Length = {len(sequence)}, Dominant Class = {dominant_class}, Max Length = {max_segment_length}")

        if len(sequence) > max_segment_length:
            for start in range(0, len(sequence), max_segment_length // 2):  
                end = min(start + max_segment_length, len(sequence))

                # Prevent zero-length splits
                if end - start > 0:
                    new_data.append(sequence[start:end])
                    new_labels.append(label_seq[start:end])

                    # 🔥 DEBUG: Print split segment info
                    print(f"✅ Split Sequence {i}: Start {start}, End {end}, Length {end - start}")

        else:
            new_data.append(sequence)
            new_labels.append(label_seq)

    # Convert lists to NumPy arrays safely
    new_data = np.array(new_data, dtype=object)  
    new_labels = np.array(new_labels, dtype=object)

    # 🔥 DEBUG: Print final dataset size
    #print(f"✅ Final Split Data Shape: {new_data.shape}, Labels Shape: {new_labels.shape}")

    return new_data, new_labels




def augment_data(
    data,
    labels_main,
    labels_prefog,
    debug=True,
    seed=None,
    add_noise_flag=False,
    add_drift_flag=False,
    add_scaling_flag=False
):
    """
    Applies optional noise, scaling, and drift augmentations to raw data.
    Supports 2D input: (num_frames, num_features).
    """

    if seed is not None:
        np.random.seed(seed)

    total_frames, num_features = data.shape
    aug_data = data.copy()

    for feature_idx in range(num_features):
        if feature_idx == 0:  # Velocity
            if add_scaling_flag:
                aug_data[:, feature_idx] = magnitude_scale(
                    aug_data[:, feature_idx], scale_range=(0.97, 1.03)
                )
            if add_noise_flag:
                aug_data[:, feature_idx] = add_noise(
                    aug_data[:, feature_idx], noise_level=0.0002
                )

        elif feature_idx == 1:  # Height
            if add_scaling_flag:
                aug_data[:, feature_idx] = magnitude_scale(
                    aug_data[:, feature_idx], scale_range=(0.99, 1.01)
                )
            if add_noise_flag:
                aug_data[:, feature_idx] = add_noise(
                    aug_data[:, feature_idx], noise_level=0.0001
                )

        elif feature_idx in [2, 3]:  # Positional Data (X, Z)
            if add_drift_flag:
                drift = np.random.uniform(-0.005, 0.005, size=aug_data[:, feature_idx].shape)
                aug_data[:, feature_idx] += drift

        elif feature_idx == 4:  # Angular Data
            if add_scaling_flag:
                aug_data[:, feature_idx] = magnitude_scale(
                    aug_data[:, feature_idx], scale_range=(0.99, 1.01)
                )
            if add_noise_flag:
                aug_data[:, feature_idx] = add_noise(
                    aug_data[:, feature_idx], noise_level=0.00005
                )

    augmented_data = aug_data.astype(np.float32)
    augmented_labels_main = labels_main.copy().astype(int)
    augmented_labels_prefog = labels_prefog.copy().astype(int)

    if debug:
        unique_classes, class_counts = np.unique(
            np.argmax(augmented_labels_main, axis=-1), return_counts=True
        )
        print("\n🔹 Augmented Class Distribution:", dict(zip(unique_classes, class_counts)))

        prefog_counts = np.bincount(augmented_labels_prefog.flatten(), minlength=2)
        print("\n🔹 PreFOG Distribution: Non-PreFOG(0) =", prefog_counts[0],
              ", PreFOG(1) =", prefog_counts[1])
        print(f"✅ Final Augmented Data Shape: {augmented_data.shape}")
        print(f"✅ Final Augmented Labels Main Shape: {augmented_labels_main.shape}")
        print(f"✅ Final Augmented Labels PreFOG Shape: {augmented_labels_prefog.shape}")

    return augmented_data, augmented_labels_main, augmented_labels_prefog



def is_pure_sequence(window_labels, target_class, class_specific_thresh=None, default_thresh=0.9):
    """
    Checks if a sequence is 'pure enough' based on a class-specific or default threshold.
    """
    thresh = class_specific_thresh.get(target_class, default_thresh) if class_specific_thresh else default_thresh
    return np.mean(window_labels == target_class) >= thresh

def is_far_from_transition(window_labels, buffer_frames=15):
    """
    Ensures the window doesn't start or end near a class transition.
    """
    transitions = np.diff(window_labels)
    return not (np.any(transitions[:buffer_frames]) or np.any(transitions[-buffer_frames:]))


def meets_basic_feature_filter(seq, cls):
    if seq.ndim == 3 and seq.shape[1] == 1:
        seq = seq[:, 0, :]
    if seq.ndim == 2 and seq.shape[1] == 1:
        seq = seq.squeeze(-1)

    if seq.ndim != 2 or seq.shape[1] < 9:
        raise ValueError(f"Expected 2D array with at least 9 features, got shape {seq.shape}")

    # --- Feature unpacking ---
    ang_vel         = seq[:, 0]
    ht_score        = seq[:, 1]
    smooth_vel      = seq[:, 2]
    displ           = seq[:, 3]
    ht_inv          = seq[:, 4]
    long_rise_sts   = seq[:, 5]
    stand_still     = seq[:, 6]
    ratio           = seq[:, 7]
    pure_walk       = seq[:, 8]

    if cls == 2:  # SitToStand
        passed = (
            np.mean(long_rise_sts) > 0.05 and
            np.max(ht_score) > 0.70 and
            np.mean(smooth_vel) > 0.06
        )

    elif cls == 3:  # StandToSit
        passed = (
            np.mean(ht_inv) > 0.32 and
            np.mean(smooth_vel) > 0.07
        )

    elif cls == 4:  # Turn In Place Left
        passed = (
            np.mean(ang_vel) < -0.09 and
            np.mean(smooth_vel) < 0.12 and
            np.mean(stand_still) < 0.4
        )

    elif cls == 5:  # Turn In Place Right
        passed = (
            np.mean(ang_vel) > 0.09 and
            np.mean(smooth_vel) < 0.12 and
            np.mean(stand_still) < 0.4
        )

    elif cls == 6:  # Turn While Walking Left
        passed = (
            np.mean(ang_vel) < -0.06 and
            np.mean(smooth_vel) > 0.50 and
            np.mean(displ) > 0.25
        )

    elif cls == 7:  # Turn While Walking Right
        passed = (
            np.mean(ang_vel) > 0.06 and
            np.mean(smooth_vel) > 0.50 and
            np.mean(displ) > 0.25
        )

    else:
        passed = True

    return passed


def oversample(
    data_long,
    labels_main_long,
    labels_prefog_long=None,
    window_size=300,
    step_size=75,
    min_class_ratio=0.2,
    target_frame_count=None,
    step_size_per_class=None,
    min_class_ratio_per_class=None,
    add_noise_flag=False,
    add_scaling_flag=False,
    add_drift_flag=False,
    debug=True
):
    # Normalize input shape to 2D if needed
    if data_long.ndim == 3 and data_long.shape[1] == 1:
        data_long = np.squeeze(data_long, axis=1)

    num_classes = labels_main_long.shape[1]
    frame_labels = np.argmax(labels_main_long, axis=-1)
    class_counts = np.bincount(frame_labels, minlength=num_classes)

    if target_frame_count is None:
        max_count = np.max(class_counts)
        target_frame_count = {cls: max_count for cls in range(num_classes)}

    class_to_windows = defaultdict(list)

    class_thresh_dict = {
        0: 0.98,  # Sitting
        1: 0.95,  # Standing
        2: 0.90,  # SitToStand
        3: 0.90,  # StandToSit
        4: 0.75,  # TurnL1
        5: 0.70,  # TurnR1
        6: 0.75,  # TurnL2
        7: 0.70,  # TurnR2
        8: 0.90   # Walking
    }

    for cls in range(num_classes):
        step = step_size_per_class.get(cls, step_size) if step_size_per_class else step
        ratio_thresh = min_class_ratio_per_class.get(cls, min_class_ratio) if min_class_ratio_per_class else min_class_ratio

        for start in range(0, len(data_long) - window_size, step):
            end = start + window_size
            window_labels = frame_labels[start:end]
            window_data = data_long[start:end]

            ratio = np.mean(window_labels == cls)
            if (
                ratio >= ratio_thresh
                and is_pure_sequence(window_labels, cls, class_specific_thresh=class_thresh_dict, default_thresh=0.9)
                and is_far_from_transition(window_labels, buffer_frames=15)
                #and meets_basic_feature_filter(window_data, cls)
            ):
                class_to_windows[cls].append((start, end))

    collected_data = [data_long]
    collected_labels_main = [labels_main_long]
    collected_labels_prefog = [labels_prefog_long] if labels_prefog_long is not None else None
    new_class_counts = class_counts.copy()
    oversampled_frame_indices = []

    current_index = len(data_long)
    for cls in range(num_classes):
        current = class_counts[cls]
        target = target_frame_count.get(cls, current)
        deficit = target - current

        if deficit <= 0 or cls not in class_to_windows:
            continue

        windows = class_to_windows[cls]
        i = 0
        while new_class_counts[cls] < target:
            start, end = windows[i % len(windows)]

            window_data = data_long[start:end]
            if window_data.ndim == 3 and window_data.shape[1] == 1:
                window_data = np.squeeze(window_data, axis=1)  # (N, 1, F) → (N, F)

            window_labels_main = labels_main_long[start:end]
            window_labels_prefog = labels_prefog_long[start:end] if labels_prefog_long is not None else np.zeros((window_size, 1))

            if add_noise_flag or add_scaling_flag or add_drift_flag:
                window_data, window_labels_main, window_labels_prefog = augment_data(
                    data=window_data,
                    labels_main=window_labels_main,
                    labels_prefog=window_labels_prefog,
                    add_noise_flag=add_noise_flag,
                    add_scaling_flag=add_scaling_flag,
                    add_drift_flag=add_drift_flag,
                    debug=False
                )

            collected_data.append(window_data)
            collected_labels_main.append(window_labels_main)
            if labels_prefog_long is not None:
                collected_labels_prefog.append(window_labels_prefog)

            oversampled_frame_indices.extend(range(current_index, current_index + window_size))
            current_index += window_size

            window_labels = frame_labels[start:end]
            for c in range(num_classes):
                new_class_counts[c] += np.sum(window_labels == c)

            i += 1

    oversampled_data = np.vstack(collected_data)
    oversampled_labels_main = np.vstack(collected_labels_main)
    oversampled_labels_prefog = (
        np.vstack(collected_labels_prefog) if labels_prefog_long is not None else None
    )

    is_augmented_mask = np.zeros(len(oversampled_data), dtype=bool)
    is_augmented_mask[oversampled_frame_indices] = True

    if debug:
        new_frame_labels = np.argmax(oversampled_labels_main, axis=-1)
        new_class_counts = np.bincount(new_frame_labels, minlength=num_classes)
        print("\n[Oversample] Frame Counts After Oversampling:")
        for cls, count in enumerate(new_class_counts):
            print(f"  Class {cls}: {count}")

    return oversampled_data, oversampled_labels_main, oversampled_labels_prefog, is_augmented_mask


def feat_eng(input_data, labels, fog_labels):
    """
    Perform feature engineering and additional preprocessing on input data.
    """
    
    def butter_lowpass_filter(data, cutoff=0.75, fs=30, order=4):
        nyquist = 0.5 * fs
        normal_cutoff = cutoff / nyquist
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        return filtfilt(b, a, data)
    
    def compute_jitter_index(ht_smoothed, window_size=15, step=1): #15

        # Third derivative
        third_derivative = np.diff(ht_smoothed, n=3)

        # Replace NaNs/Infs in third derivative
        if np.isnan(third_derivative).all():
            third_derivative = np.zeros_like(third_derivative)
        else:
            third_derivative = np.nan_to_num(third_derivative, nan=np.nanmean(third_derivative))

        jitter_vals = []
        for start in range(0, len(third_derivative) - window_size + 1, step):
            window = third_derivative[start : start + window_size]
            if np.isnan(window).any() or np.isinf(window).any():
                window = np.nan_to_num(window, nan=0.0, posinf=0.0, neginf=0.0)
            jitter = np.sqrt(np.mean(window**2))  # RMS
            jitter_vals.append(jitter)

        # Pad beginning to align output length
        padding = [jitter_vals[0]] * (window_size + 2)  # +2 for 3rd derivative shift
        jitter_feature = np.array(padding + jitter_vals)

        # Final cleanup
        jitter_feature = np.nan_to_num(jitter_feature, nan=0.0, posinf=0.0, neginf=0.0)

        return jitter_feature

    def baseline_scale(feature):
        p_min = np.percentile(feature, 0)
        p_max = np.percentile(feature, 100)
        denom = p_max - p_min if p_max - p_min != 0 else 1.0
        return (np.clip((feature - p_min) / denom, 0, 1))*10
    
    epsilon = 1e-8
    debug=True
    
    # Handle full-length data
    total_frames = input_data.shape[0]
    num_features = input_data.shape[1]

    # Extract features
    velocity_XZ = np.nan_to_num(input_data[:, 0], nan=0.0, posinf=0.0, neginf=0.0)
    ht = np.nan_to_num(input_data[:, 1], nan=0.0, posinf=0.0, neginf=0.0)
    cam_x = np.nan_to_num(input_data[:, 2], nan=0.0, posinf=0.0, neginf=0.0)
    cam_z = np.nan_to_num(input_data[:, 3], nan=0.0, posinf=0.0, neginf=0.0)
    cam_ang = np.nan_to_num(input_data[:, 4], nan=0.0, posinf=0.0, neginf=0.0)
    
    # Height
    ht_smoothed = gaussian_filter1d(ht, sigma=1)
    
    # Ensure cam_ang is 2D
    if cam_ang.ndim == 1:
        cam_ang = cam_ang.reshape(-1, 1)

    # --- Circular Angle Delta ---
    raw_angle_diff = np.diff(cam_ang, axis=0, prepend=cam_ang[0:1, :])
    sin_component = np.sin(np.radians(raw_angle_diff))
    cos_component = np.cos(np.radians(raw_angle_diff))
    delta_angle_rad = np.arctan2(sin_component, cos_component)
    delta_angle = np.degrees(delta_angle_rad).flatten()

    # --- Hard Deadzone for Tiny Motion (critical) ---
    tiny_delta_threshold = 0.3  # degrees per frame (can test 0.3–0.7) #0.3
    delta_angle[np.abs(delta_angle) < tiny_delta_threshold] = 0.0

    # --- Mild Butterworth Filtering ---
    delta_angle_smoothed = butter_lowpass_filter(delta_angle, cutoff=0.3, fs=30, order=3)

    # --- Rolling Mean to Bridge Nearby Motion ---
    delta_angle_smoothed = pd.Series(delta_angle_smoothed).rolling(window=15, center=True, min_periods=1).mean().values #15

    hard_thresh = 0.03
    ang_vel_smoothed = np.where(
        np.abs(delta_angle_smoothed) >= hard_thresh,
        delta_angle_smoothed,
        0.0
    )

    ang_vel = np.clip(ang_vel_smoothed, -1.0, 1.0)

    ang_vel_smoothed = np.nan_to_num(ang_vel, nan=0.0, posinf=0.0, neginf=0.0)
    
    ang_vel_sm_l = (ang_vel_smoothed < -0.15).astype(float) * ang_vel_smoothed 
    ang_vel_sm_r = (ang_vel_smoothed >  0.25).astype(float) * ang_vel_smoothed 
    ang_vel_sm_l *=-1
    ang_vel_sm_l = ang_vel_sm_l >0.4
    ang_vel_sm_r = ang_vel_sm_r >0.4 

    velocity_XZ = np.nan_to_num(velocity_XZ, nan=0.0, posinf=0.0, neginf=0.0)

    # Smooth with small rolling window
    smooth_velocity = pd.Series(velocity_XZ).rolling(window=5, center=True, min_periods=1).mean().to_numpy()

    # Ensure no gaps after smoothing
    smooth_velocity = pd.Series(smooth_velocity).interpolate(method='linear', limit_direction='both').fillna(0.0).to_numpy()

    smooth_velocity = np.clip(smooth_velocity, 0, 1)
    smooth_velocity = smooth_velocity
    
    jitter = compute_jitter_index(ht_smoothed)
    jitter = baseline_scale(jitter)
    jitter = jitter * smooth_velocity
    jitter = jitter * (ht_smoothed > 0.21)
    jitter = np.clip(jitter, 0, 1)

    fog_posture_signal = np.gradient(ht_smoothed, 1/30)
    fog_posture_signal = fog_posture_signal * (ht_smoothed > 0.21)
    fog_posture_signal = fog_posture_signal * ht_smoothed
    fog_posture_signal = np.clip(fog_posture_signal, -1, 1)
    fog_posture_signal = fog_posture_signal**2

    
    # ✅ Define Features
    features_to_fix = [
        ht_smoothed,
        ang_vel_sm_l,
        ang_vel_sm_r,
        smooth_velocity,
        jitter,
        fog_posture_signal
        
    ]

    # ✅ Convert all features to NumPy arrays
    features_to_fix = [np.asarray(f) for f in features_to_fix]

    # ✅ Ensure all features have the same shape (N,1)
    for i in range(len(features_to_fix)):
        if features_to_fix[i].ndim == 1:
            features_to_fix[i] = features_to_fix[i].reshape(-1, 1)
        elif features_to_fix[i].ndim > 2:
            features_to_fix[i] = features_to_fix[i].squeeze().reshape(-1, 1)

    # ✅ Debug: Verify final feature shapes
    expected_shape = features_to_fix[0].shape
    for i, f in enumerate(features_to_fix):
        if f.shape != expected_shape:
            print(f"Shape Mismatch: Feature {i} has {f.shape}, expected {expected_shape}")
            features_to_fix[i] = np.resize(f, expected_shape)  # Resize if necessary

    # ✅ Debug Output
    print(f"✅ All features ready for stacking with shape: {expected_shape}")

    # ✅ Stack
    stacked_data = np.column_stack(features_to_fix)

    # ✅ Final shape check
    print(f"✅ Stacked Data Shape: {stacked_data.shape}")  # Should be (N, feature_count)

    # ✅ Unpack fixed features
    (
        ht_smoothed,
        ang_vel_sm_l,
        ang_vel_sm_r,
        smooth_velocity,
        jitter,
        fog_posture_signal
        
    ) = features_to_fix


    augmented_data = np.stack([

        ht_smoothed,
        ang_vel_sm_l,
        ang_vel_sm_r,
        smooth_velocity
#         jitter,
#         fog_posture_signal
        
    ], axis=-1)
    

    print(f"✅ Final augmented_data shape: {augmented_data.shape}")

    # ✅ Debugging: Print the first 20 values for each feature after stacking
    print("\n🔍 First 20 Values of Each Feature After Stacking:")


    # Ensure correct shape before iterating over features
    augmented_data_squeezed = np.squeeze(augmented_data)  # Remove extra dimensions if (N, 1, 6)

    print(f"🔍 Augmented Data Shape (Before Squeeze): {augmented_data.shape}")
    print(f"🔍 Augmented Data Shape (After Squeeze): {augmented_data_squeezed.shape}")

    if augmented_data_squeezed.ndim == 2:  # Ensure valid shape before accessing features
        num_features = augmented_data_squeezed.shape[1]  # Extract valid feature count
        print(f"🔢 Total detected features: {num_features}")

        for i in range(num_features):  # Iterate over features safely
            print(f"\nFeature {i} (First 20 values):")
            print(augmented_data_squeezed[:20, i])  # Print first 20 values
    else:
        print(f"⚠️ Unexpected shape for augmented_data_squeezed: {augmented_data_squeezed.shape}")

    # ✅ Check for NaNs in individual features
    print(f"🟠 Final feature stack - NaN count: {np.isnan(augmented_data_squeezed).sum()}")

    # ✅ Debugging: Print feature statistics
        # ✅ Debugging: Print feature statistics
    if debug:
        print("\nApply_Advanced_Aug Feature Statistics:")
        # Ensure augmented_data_squeezed is 2D
        if augmented_data_squeezed.ndim == 1:
            augmented_data_squeezed = augmented_data_squeezed.reshape(-1, 1)  # Convert to (X, 1)

        # Dynamically set num_features
        num_features = augmented_data_squeezed.shape[1]  

        for i in range(num_features):  
            feature = augmented_data_squeezed[:, i]  

            # Compute statistics
            min_val = np.min(feature)
            max_val = np.max(feature)
            mean_val = np.mean(feature)
            std_val = np.std(feature)
            median_val = np.median(feature)
            skewness = skew(feature)
            kurt = kurtosis(feature)
            iqr = np.percentile(feature, 75) - np.percentile(feature, 25)

            # Print results
            print(f"Feature {i}: min={min_val:.4f}, max={max_val:.4f}, mean={mean_val:.4f}, std={std_val:.4f}")
            print(f"           median={median_val:.4f}, IQR={iqr:.4f}, skew={skewness:.4f}, kurtosis={kurt:.4f}")

    # ✅ Plot each feature separately with FOG overlay
        
    # Motor State Indices:
    {'Sitting': 0, 'Standing': 1, 'SitToStand': 2, 'StandToSit': 3, 
     'TurnL': 4, 'TurnR': 5, 'TurnL2': 6, 'TurnR2': 7, 'Walking': 8, 'GI': 9, 'GT': 10, 'FOG': 11, 'predFOG': 12}
    
    # Extract Labels (Update GT_index to your correct column)
    # Ensure labels has the correct shape
    gt_labels = np.argmax(labels, axis=1)  # Convert one-hot labels to single indices (0-7)
    ht_scaled = ht_smoothed

    if debug:
        # Define start index and number of timesteps to plot
        start_idx = 4500  # Adjust as needed
        num_timesteps = 8750  # Adjust as needed
        step = 1  # Adjust step size for downsampling

        # Ensure indices are within bounds
        end_idx = min(start_idx + num_timesteps, len(augmented_data_squeezed))
        start_idx = max(0, start_idx)

        # Extract the selected range
        selected_data = augmented_data_squeezed[start_idx:end_idx:step]
        selected_labels_gt = gt_labels[start_idx:end_idx:step]
        selected_labels_fog = fog_labels[start_idx:end_idx:step]  # Extract FOG labels

        print("\n🔍 Generating feature plots with GT Motor State Line...")

        # ✅ Verify GT labels distribution
        unique_labels, counts = np.unique(selected_labels_gt, return_counts=True)
        print("GT Labels Distribution (Selected Range):", dict(zip(unique_labels, counts)))

        # ✅ Verify FOG labels distribution
        unique_fog_labels, fog_counts = np.unique(selected_labels_fog, return_counts=True)
        print("FOG Labels Distribution (Selected Range):", dict(zip(unique_fog_labels, fog_counts)))

        # Plot each feature with the GT Motor State overlay
        for i in range(num_features):
            plt.figure(figsize=(12, 6))
            plt.plot(selected_data[:, i], label=f"Feature {i}", alpha=0.8, color="blue")

            # 🔹 **Plot GT Motor State as a Continuous Stepped Line**
            plt.step(range(len(selected_labels_gt)), selected_labels_gt, 
                     where='post', color="black", linewidth=2, label="GT Motor State (0-12)")

            plt.title(f"Feature {i} (Samples {start_idx} to {end_idx})")
            plt.xlabel("Time Step")
            plt.ylabel("Value")
            plt.grid(True)
            plt.legend()
            plt.show()

        print("\n🔍 Generating feature plots with FOG Labels Line...")

        # Plot each feature with the FOG label overlay
        for i in range(num_features):
            plt.figure(figsize=(12, 6))
            plt.plot(selected_data[:, i], label=f"Feature {i}", alpha=0.8, color="blue")

            # 🔹 **Plot FOG Labels as a Continuous Stepped Line**
            plt.step(range(len(selected_labels_fog)), selected_labels_fog, 
                     where='post', color="red", linewidth=2, label="FOG Label (0=No FOG, 1=FOG)")

            plt.title(f"Feature {i} (Samples {start_idx} to {end_idx}) - FOG Overlay")
            plt.xlabel("Time Step")
            plt.ylabel("Value")
            plt.grid(True)
            plt.legend()
            plt.show()


        # ✅ Overlay all features in a single plot
        print("\n🔍 Overlaying all features with GT Motor State Line...")

        plt.figure(figsize=(12, 6))

        for i in range(num_features):
            plt.plot(selected_data[:, i], alpha=0.5, label=f"Feature {i}")

        # 🔹 **Overlay GT Motor State**
        plt.step(range(len(selected_labels_gt)), selected_labels_gt, 
                 where='post', color="black", linewidth=2, label="GT Motor State (0-12)")

        plt.title(f"All Features (Samples {start_idx} to {end_idx}) with GT Motor State Overlay")
        plt.xlabel("Time Step")
        plt.ylabel("Value")
        plt.grid(True)
        plt.legend()
        plt.show()


        # Debugging: Print shapes of key arrays
        print(f"Final Augmented Data Shape: {augmented_data.shape}")
        print(f"Labels Shape: {labels.shape}")

        # 📌 Check augmented_data shape before printing
        print(f"\n🔍 Augmented data shape: {augmented_data.shape}")
        num_features = augmented_data.shape[-1]  # Get actual number of features
        print(f"Total detected features: {num_features}")
        
        # ✅ Ensure GT labels are categorical indices
        gt_labels_categorical = labels.argmax(axis=-1)

        # ✅ Identify indices for all motor states
        labeled_sit = np.where(gt_labels_categorical == 0)[0]
        labeled_stand = np.where(gt_labels_categorical == 1)[0]
        labeled_sts = np.where(gt_labels_categorical == 2)[0]
        labeled_stsit = np.where(gt_labels_categorical == 3)[0]
        labeled_turnL = np.where(gt_labels_categorical == 4)[0]
        labeled_turnR = np.where(gt_labels_categorical == 5)[0]
        labeled_turnL2 = np.where(gt_labels_categorical == 6)[0]
        labeled_turnR2 = np.where(gt_labels_categorical == 7)[0]
        labeled_walk = np.where(gt_labels_categorical == 8)[0]
        labeled_fog =  np.where(gt_labels_categorical == 11)[0]
        
        print("🔎 Unique classes in gt_labels_categorical:", np.unique(gt_labels_categorical))
        print("💡 Sample indices where class 11 appears:", np.where(gt_labels_categorical == 11)[0][:10])

        

        # ✅ Print total GT frames per category
        print("\n🔍 GT Frame Counts per State:")
        print(f"  - Sitting (Class 0): {len(labeled_sit)}")
        print(f"  - Standing (Class 1): {len(labeled_stand)}")
        print(f"  - Sit-to-Stand (Class 2): {len(labeled_sts)}")
        print(f"  - Stand-to-Sit (Class 3): {len(labeled_stsit)}")
        print(f"  - Turning Left (Class 4): {len(labeled_turnL)}")
        print(f"  - Turning Right (Class 5): {len(labeled_turnR)}")
        print(f"  - Turning Left2 (Class 6): {len(labeled_turnL2)}")
        print(f"  - Turning Right2 (Class 7): {len(labeled_turnR2)}")
        print(f"  - Walking (Class 8): {len(labeled_walk)}")
        print(f"  - FOG(Class 11): {len(labeled_fog)}")

    
        # Extract height values where GT labels indicate standing-based states
        standing_ht_values = ht_scaled[(gt_labels == 1) | (gt_labels == 4) | (gt_labels == 5) | (gt_labels == 6)| (gt_labels == 7) | (gt_labels == 8)]

        # Print key statistics
        print("Standing-Based Height Values:")
        print("Min:", np.min(standing_ht_values))
        print("Max:", np.max(standing_ht_values))
        print("Median:", np.median(standing_ht_values))
        
        # Heights for only pure standing
        pure_standing_ht = ht_scaled[gt_labels == 1]
        pure_sitting_ht = ht_scaled[gt_labels == 0]

        # Heights for turning (left & right)
        turning_ht = ht_scaled[(gt_labels == 4) | (gt_labels == 5)]
        turning2_ht = ht_scaled[(gt_labels == 6) | (gt_labels == 7)]

        # Heights for walking
        walking_ht = ht_scaled[gt_labels == 8]
        
        sts_ht = ht_scaled[gt_labels == 2]
        
        stsit_ht = ht_scaled[gt_labels == 3]
        
        fog_ht = ht_scaled[gt_labels == 11]

        # Print stats for each  
        print("\nPure Sitting Heights:")
        print("Min:", np.min(pure_sitting_ht))
        print("Max:", np.max(pure_sitting_ht))
        print("Median:", np.median(pure_sitting_ht))
        print("10th Percentile - Thresh:", np.percentile(pure_sitting_ht, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(pure_sitting_ht, 90))
        
        print("\nPure Standing Heights:")
        print("Min:", np.min(pure_standing_ht))
        print("Max:", np.max(pure_standing_ht))
        print("Median:", np.median(pure_standing_ht))
        print("10th Percentile - Thresh:", np.percentile(pure_standing_ht, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(pure_standing_ht, 90))
        
        print("\nSTS Heights:")
        print("Min:", np.min(sts_ht))
        print("Max:", np.max(sts_ht))
        print("Median:", np.median(sts_ht))
        print("10th Percentile - Thresh:", np.percentile(sts_ht, 10))
        
        print("\nPure STSit Heights:")
        print("Min:", np.min(stsit_ht))
        print("Max:", np.max(stsit_ht))
        print("Median:", np.median(stsit_ht))
        print("90th Percentile - Upper Thresh:", np.percentile(stsit_ht, 90))

        print("\nTurning Heights:")
        print("Min:", np.min(turning_ht))
        print("Max:", np.max(turning_ht))
        print("Median:", np.median(turning_ht))
        print("10th Percentile - Thresh:", np.percentile(turning_ht, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(turning_ht, 90))
        
        print("\nTurning2 Heights:")
        print("Min:", np.min(turning2_ht))
        print("Max:", np.max(turning2_ht))
        print("Median:", np.median(turning2_ht))
        print("10th Percentile - Thresh:", np.percentile(turning2_ht, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(turning2_ht, 90))

        print("\nWalking Heights:")
        print("Min:", np.min(walking_ht))
        print("Max:", np.max(walking_ht))
        print("Median:", np.median(walking_ht))
        print("10th Percentile - Thresh:", np.percentile(walking_ht, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(walking_ht, 90))
        
        print("\nFOG Heights:")
        print("Min:", np.min(fog_ht))
        print("Max:", np.max(fog_ht))
        print("Median:", np.median(fog_ht))
        print("10th Percentile - Thresh:", np.percentile(fog_ht, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(fog_ht, 90))
        
        # Extract velocity values for each category from GT labels
        sitting_velocity = smooth_velocity[gt_labels == 0]
        standing_velocity = smooth_velocity[gt_labels == 1]
        sts_velocity = smooth_velocity[gt_labels == 2]
        stsit_velocity = smooth_velocity[gt_labels == 3]
        walking_velocity = smooth_velocity[gt_labels == 8]
        turning_velocity = smooth_velocity[(gt_labels == 4) | (gt_labels == 5)]
        turning2_velocity = smooth_velocity[(gt_labels == 6) | (gt_labels == 7)]

        # Extract angular velocity values for turning-based activities
        turning_ang_vel = ang_vel_smoothed[(gt_labels == 4) | (gt_labels == 5)]
        turning2_ang_vel = ang_vel_smoothed[(gt_labels == 6) | (gt_labels == 7)]
        
#         #long_drop
#         stsit_drop = long_rise[gt_labels == 3]
        
#         #long_rise
#         sts_rise = long_rise[gt_labels == 2]
        
        # Print key statistics for velocity
        print("\nSitting Velocity Stats:")
        print("Min:", np.min(sitting_velocity))
        print("Max:", np.max(sitting_velocity))
        print("Median:", np.median(sitting_velocity))
        
        print("\nSTS Velocity Stats:")
        print("Min:", np.min(sts_velocity))
        print("Max:", np.max(sts_velocity))
        print("Median:", np.median(sts_velocity))
        
        print("\nSTSit Velocity Stats:")
        print("Min:", np.min(stsit_velocity))
        print("Max:", np.max(stsit_velocity))
        print("Median:", np.median(stsit_velocity))
        
        print("\nStanding Velocity Stats:")
        print("Min:", np.min(standing_velocity))
        print("Max:", np.max(standing_velocity))
        print("Median:", np.median(standing_velocity))
        print("10th Percentile - Thresh:", np.percentile(standing_velocity, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(standing_velocity, 90))
        
        print("\nWalking Velocity Stats:")
        print("Min:", np.min(walking_velocity))
        print("Max:", np.max(walking_velocity))
        print("Median:", np.median(walking_velocity))
        print("10th Percentile - Thresh:", np.percentile(walking_velocity, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(walking_velocity, 90))

        print("\nTurning Velocity Stats:")
        print("Min:", np.min(turning_velocity))
        print("Max:", np.max(turning_velocity))
        print("Median:", np.median(turning_velocity))
        print("10th Percentile - Thresh:", np.percentile(turning_velocity, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(turning_velocity, 90))
        
        print("\nTurning2 Velocity Stats:")
        print("Min:", np.min(turning2_velocity))
        print("Max:", np.max(turning2_velocity))
        print("Median:", np.median(turning2_velocity))
        print("10th Percentile - Thresh:", np.percentile(turning2_velocity, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(turning2_velocity, 90))

        # Print key statistics for angular velocity
        print("\nTurning Angular Velocity Stats:")
        print("Min:", np.min(turning_ang_vel))
        print("Max:", np.max(turning_ang_vel))
        print("Median:", np.median(turning_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(turning_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(turning_ang_vel, 90))
        
        # Print key statistics for angular velocity
        print("\nTurning2 Angular Velocity Stats:")
        print("Min:", np.min(turning2_ang_vel))
        print("Max:", np.max(turning2_ang_vel))
        print("Median:", np.median(turning2_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(turning2_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(turning2_ang_vel, 90))

        # Extract angular velocity for left and right turns separately
        standing_ang_vel = ang_vel_smoothed[gt_labels == 1]
        left_turn_ang_vel = ang_vel_smoothed[gt_labels == 4]  # GT Left Turns
        right_turn_ang_vel = ang_vel_smoothed[gt_labels == 5]  # GT Right Turns
        left_turn2_ang_vel = ang_vel_smoothed[gt_labels == 6]  # GT Left Turns
        right_turn2_ang_vel = ang_vel_smoothed[gt_labels == 7]  # GT Right Turns
        walking_ang_vel = ang_vel_smoothed[gt_labels == 8]

        # Compute stats
        
        print("\nStanding Angular Velocity Stats:")
        print("Min:", np.min(standing_ang_vel))
        print("Max:", np.max(standing_ang_vel))
        print("Median:", np.median(standing_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(standing_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(standing_ang_vel, 90))
        
        print("\nLeft Turn Angular Velocity Stats:")
        print("Min:", np.min(left_turn_ang_vel))
        print("Max:", np.max(left_turn_ang_vel))
        print("Median:", np.median(left_turn_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(left_turn_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(left_turn_ang_vel, 90))
        
        print("\nRight Turn Angular Velocity Stats:")
        print("Min:", np.min(right_turn_ang_vel))
        print("Max:", np.max(right_turn_ang_vel))
        print("Median:", np.median(right_turn_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(right_turn_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(right_turn_ang_vel, 90))
        
        print("\nLeft Turn2 Angular Velocity Stats:")
        print("Min:", np.min(left_turn2_ang_vel))
        print("Max:", np.max(left_turn2_ang_vel))
        print("Median:", np.median(left_turn2_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(left_turn2_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(left_turn2_ang_vel, 90))

        print("\nRight Turn2 Angular Velocity Stats:")
        print("Min:", np.min(right_turn2_ang_vel))
        print("Max:", np.max(right_turn2_ang_vel))
        print("Median:", np.median(right_turn2_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(right_turn2_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(right_turn2_ang_vel, 90))
        
        print("\nWalk Angular Velocity Stats:")
        print("Min:", np.min(walking_ang_vel))
        print("Max:", np.max(walking_ang_vel))
        print("Median:", np.median(walking_ang_vel))
        print("10th Percentile - Thresh:", np.percentile(walking_ang_vel, 10))
        print("90th Percentile - Upper Thresh:", np.percentile(walking_ang_vel, 90))
        
    return augmented_data, labels


def oversample_prefog(train_data, train_labels_prefog, train_labels_main, 
                      train_seated_heights, train_standing_heights,
                      oversample_factor=1.5):
    """Oversample PreFOG (1) samples while ensuring alignment across all related datasets."""
    
    # ✅ Find indices of PreFOG (1) samples
    prefog_indices = np.where(train_labels_prefog.flatten() == 1)[0] 

    if len(prefog_indices) == 0:
        print("⚠️ No PreFOG samples found in training set!")
        return train_data, train_labels_prefog, train_labels_main, train_seated_heights, train_standing_heights

    # ✅ Determine how many new PreFOG samples to generate
    new_prefog_count = int(len(prefog_indices) * (oversample_factor - 1))
    
    # ✅ Sample with replacement
    duplicated_indices = np.random.choice(prefog_indices, new_prefog_count, replace=True)

    # ✅ Ensure all datasets remain aligned
    train_data_oversampled = np.concatenate([train_data, train_data[duplicated_indices]], axis=0)
    train_labels_prefog_oversampled = np.concatenate([train_labels_prefog, train_labels_prefog[duplicated_indices]], axis=0)
    train_labels_main_oversampled = np.concatenate([train_labels_main, train_labels_main[duplicated_indices]], axis=0)
    train_seated_heights_oversampled = np.concatenate([train_seated_heights, train_seated_heights[duplicated_indices]], axis=0)
    train_standing_heights_oversampled = np.concatenate([train_standing_heights, train_standing_heights[duplicated_indices]], axis=0)

    print(f"✅ Oversampled PreFOG from {len(prefog_indices)} → {len(train_labels_prefog_oversampled)} samples")

    return (train_data_oversampled, train_labels_prefog_oversampled, train_labels_main_oversampled, 
            train_seated_heights_oversampled, train_standing_heights_oversampled)


def compute_class_distribution(labels):
    """
    Computes the class distribution (counts and frequencies) from one-hot encoded labels.
    
    Parameters:
        labels (np.array): One-hot encoded labels of shape (num_samples, num_classes).
    
    Returns:
        class_counts (dict): Dictionary with class indices as keys and sample counts as values.
        class_frequencies (dict): Dictionary with class indices as keys and relative frequencies as values.
    """
    # Convert one-hot encoded labels to categorical indices
    class_indices = np.argmax(labels, axis=-1)

    # Compute class counts
    unique_classes, counts = np.unique(class_indices, return_counts=True)
    class_counts = dict(zip(unique_classes, counts))

    # Compute total samples
    total_samples = sum(class_counts.values())

    # Compute class frequencies
    class_frequencies = {k: v / total_samples for k, v in class_counts.items()}

    return class_counts, class_frequencies


###############################################################################
#                             Model-Building Functions                        #
###############################################################################


def build_lstm_model(
    input_shape,
    output_units_locomotion,
    output_units_turn_dir,
    output_units_predfog,
    dropout_rate=0.3,
    bias_initializers=None
):
    """
    Builds a robust multi-head LSTM model suitable for ONNX export and improved generalization.
    Includes deeper, regularized heads for FOG and preFOG detection.
    """

    def binit(head_name):
        return (bias_initializers or {}).get(head_name, "zeros")

    reg = regularizers.l2(1e-4)  # L2 regularization for FOG-related heads

    # ── Input ───────────────────────────────────────────────
    inputs = tf.keras.Input(shape=input_shape)

    # ── Shared Encoder (BiLSTM + Dropout) ───────────────────
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(inputs)
    x = tf.keras.layers.Dropout(dropout_rate)(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)
    x = tf.keras.layers.Dropout(dropout_rate)(x)
    x = tf.keras.layers.BatchNormalization()(x)

    # ── Shared Dense Projection ─────────────────────────────
    x_shared = tf.keras.layers.Dense(64, activation='relu', kernel_initializer="he_normal")(x)

    # ── Output Heads ────────────────────────────────────────
    outputs = []

    # Locomotion Head
    x_loco = tf.keras.layers.Dense(32, activation='relu', kernel_initializer="he_normal")(x_shared)
    outputs.append(tf.keras.layers.Dense(
        output_units_locomotion,
        activation="softmax",
        kernel_initializer="he_normal",
        bias_initializer=binit("locomotion_output"),
        name="locomotion_output"
    )(x_loco))

    # Turn Direction Head
    x_tdir = tf.keras.layers.Dense(32, activation='relu', kernel_initializer="he_normal")(x_shared)
    outputs.append(tf.keras.layers.Dense(
        output_units_turn_dir,
        activation="softmax",
        kernel_initializer="he_normal",
        bias_initializer=binit("turn_direction_output"),
        name="turn_direction_output"
    )(x_tdir))

    # PreFOG Head (Deeper + Conv1D Smoother + Regularized)
    x_predfog = tf.keras.layers.Conv1D(32, kernel_size=5, padding="same", activation="relu")(x_shared)
    x_predfog = tf.keras.layers.Dropout(dropout_rate)(x_predfog)
    x_predfog = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=reg)(x_predfog)
    x_predfog = tf.keras.layers.Dropout(dropout_rate)(x_predfog)

    outputs.append(tf.keras.layers.Dense(
        1,
        activation="sigmoid",
        kernel_initializer="he_normal",
        kernel_regularizer=reg,
        bias_initializer=binit("pred_fog_output") if "pred_fog_output" in (bias_initializers or {})
                        else tf.keras.initializers.Constant(-1.8),
        name="pred_fog_output"
    )(x_predfog))

    # ── Final Model ─────────────────────────────────────────
    return tf.keras.Model(inputs=inputs, outputs=outputs)



# def build_lstm_model(input_shape, output_units_locomotion, output_units_turn_dir, output_units_predfog, dropout_rate=0.3):
#     inputs = tf.keras.Input(shape=input_shape)

#     # Encoder
#     x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(96, return_sequences=True))(inputs)
#     x = tf.keras.layers.Dropout(dropout_rate)(x)
#     x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(48, return_sequences=True))(x)
#     x = tf.keras.layers.Dropout(dropout_rate)(x)
#     x = tf.keras.layers.BatchNormalization()(x)

#     # Locomotion (simple)
#     x_loco = tf.keras.layers.Dense(32, activation='relu')(x)
#     locomotion_output = tf.keras.layers.Dense(output_units_locomotion, activation='softmax', name="locomotion_output")(x_loco)

#     # Turn Direction (simple)
#     turn_direction_output = tf.keras.layers.Dense(output_units_turn_dir, activation='softmax', name="turn_direction_output")(x)

#     # predFOG (deeper + regularized)
#     x_fog = tf.keras.layers.Conv1D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)
#     x_fog = tf.keras.layers.Dense(64, activation='relu')(x_fog)
#     x_fog = tf.keras.layers.Dropout(dropout_rate)(x_fog)
#     x_fog = tf.keras.layers.Dense(32, activation='relu')(x_fog)
#     pred_fog_output = tf.keras.layers.Dense(1, activation='sigmoid', name="pred_fog_output")(x_fog)

#     return tf.keras.Model(inputs=inputs, outputs=[locomotion_output, turn_direction_output, pred_fog_output])



###############################################################################
#                                 Custom Losses                                #
###############################################################################


def weighted_binary_loss(y_true, y_pred, **kwargs):
    """
    Custom weighted binary cross-entropy loss function for FOG detection.
    """
    pos_weight = 3.0  # Increase weighting for positive (FOG) class

    # ✅ Ensure tensors are correct dtype
    y_true = tf.cast(y_true, dtype=tf.float32)

    # ✅ Compute Binary Crossentropy (Already Tensor)
    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)
    unweighted_loss = bce(y_true, y_pred)  # Shape: (batch_size, time_steps)

    # ✅ Assign Higher Weight to Positive (FOG) Class
    weights = tf.where(tf.equal(y_true, 1), pos_weight, 1.0)

    # ✅ Ensure Weights Have Correct Shape (Flatten Last Dimension)
    weights = tf.squeeze(weights, axis=-1)  # Remove unnecessary last dimension if needed
    weights = tf.broadcast_to(weights, tf.shape(unweighted_loss))  # Ensure correct broadcasting

    # ✅ Normalize by Total Weights Instead of Mean
    weighted_loss = weights * unweighted_loss
    return tf.reduce_sum(weighted_loss) / (tf.reduce_sum(weights) + 1e-6)  # Prevent division by zero


class DynamicWeightedBinaryLoss(tf.keras.losses.Loss):
    def __init__(
        self,
        initial_pos_weight=2.5,
        final_pos_weight=1.25,
        gamma=1.0,
        max_epochs=50,
        current_epoch=0,
        fp_penalty_weight=1.0,
        name="dynamic_weighted_binary_loss",
        reduction=tf.keras.losses.Reduction.AUTO,
        **kwargs
    ):
        super().__init__(name=name, reduction=reduction, **kwargs)
        self.initial_pos_weight = initial_pos_weight
        self.final_pos_weight = final_pos_weight
        self.gamma = gamma
        self.max_epochs = max_epochs
        self.current_epoch = current_epoch
        self.fp_penalty_weight = fp_penalty_weight
        self.bce = tf.keras.losses.BinaryCrossentropy(
            from_logits=False,
            reduction=tf.keras.losses.Reduction.NONE
        )

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-6, 1.0 - 1e-6)

        if len(y_true.shape) == 2:
            y_true = tf.expand_dims(y_true, axis=-1)
        if len(y_pred.shape) == 2:
            y_pred = tf.expand_dims(y_pred, axis=-1)

        unweighted_loss = self.bce(y_true, y_pred)
        unweighted_loss = tf.expand_dims(unweighted_loss, axis=-1)

        pos_weight = self.final_pos_weight + (self.initial_pos_weight - self.final_pos_weight) * \
                     (1.0 - self.current_epoch / self.max_epochs)
        weights = tf.where(tf.equal(y_true, 1.0), pos_weight, 1.0)

        pt = tf.stop_gradient(tf.where(tf.equal(y_true, 1.0), y_pred, 1.0 - y_pred))
        focal_weight = tf.pow(1.0 - pt, self.gamma)

        fp_penalty = tf.cast(tf.equal(y_true, 0.0), tf.float32) * tf.pow(y_pred, 3)

        weighted_loss = weights * focal_weight * unweighted_loss + self.fp_penalty_weight * fp_penalty
        return tf.reduce_sum(weighted_loss) / (tf.reduce_sum(weights) + 1e-6)


    def set_epoch(self, epoch):
        self.current_epoch = epoch

    def get_config(self):
        return {
            "initial_pos_weight": self.initial_pos_weight,
            "final_pos_weight": self.final_pos_weight,
            "gamma": self.gamma,
            "max_epochs": self.max_epochs,
            "current_epoch": self.current_epoch,
            "name": self.name
        }

    @classmethod
    def from_config(cls, config):
        return cls(**config)



class SoftmaxFocalLoss(tf.keras.losses.Loss):
    def __init__(self, gamma=2.0, class_weights=None, reduction=tf.keras.losses.Reduction.AUTO, name='SoftmaxFocalLoss'):
        super().__init__(reduction=reduction, name=name)
        self.gamma = gamma
        self.class_weights = class_weights

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.int32)  # Ensure int for one-hot
        y_pred = tf.clip_by_value(y_pred, 1e-6, 1.0 - 1e-6)

        # ⚠️ Convert to one-hot (assumes num_classes = last dim of y_pred)
        num_classes = tf.shape(y_pred)[-1]
        y_true_one_hot = tf.one_hot(y_true, depth=num_classes)  # shape: (B, T, C)

        ce_loss = -y_true_one_hot * tf.math.log(y_pred)

        if self.class_weights is not None:
            weights = tf.constant(self.class_weights, dtype=tf.float32)
            weight_map = tf.reduce_sum(y_true_one_hot * weights, axis=-1, keepdims=True)
            ce_loss = ce_loss * weight_map

        if self.gamma > 0.0:
            pt = tf.reduce_sum(y_true_one_hot * y_pred, axis=-1, keepdims=True)
            focal_weight = tf.pow(1.0 - pt, self.gamma)
            ce_loss *= focal_weight

        return tf.reduce_mean(tf.reduce_sum(ce_loss, axis=-1))  # sum over classes, mean over batch/time


    
    
def masked_categorical_crossentropy(y_true, y_pred, fog_mask, weight_non_fog=1.0, weight_fog=0.3):
    weights = tf.where(tf.squeeze(fog_mask, axis=-1) == 1, weight_fog, weight_non_fog)
    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
    return tf.reduce_mean(loss * weights)


###############################################################################
#                            Label Smoothing Functions                         #
###############################################################################


def smooth_labels_class_specific(labels, class_smoothing_factors=None):
    """
    Apply class-specific label smoothing for both multi-class (one-hot) and binary (0/1) labels.
    """
    # Basic checks
    if labels.ndim not in [2, 3]:
        raise ValueError(f"Labels must have 2 or 3 dimensions, got {labels.ndim}.")

    if class_smoothing_factors is None or all(v == 0 for v in class_smoothing_factors.values()):
        print("Label smoothing is disabled. Returning original labels.")
        return labels

    num_classes = labels.shape[-1]

    if num_classes == 1:  
        # 🟢 **Binary classification case (predFOG)**
        smoothing_factor = class_smoothing_factors.get(1, 0.0)  # Get smoothing factor for positive class (FOG=1)
        smoothed_labels = labels * (1 - smoothing_factor) + (smoothing_factor * 0.5)
    
    else:  
        # 🔵 **Multi-class case (e.g., motor states)**
        class_indices = np.arange(num_classes)
        smoothing_matrix = np.array([class_smoothing_factors.get(i, 0.0) for i in class_indices])

        # Apply smoothing
        smoothed_labels = labels * (1 - smoothing_matrix) + (smoothing_matrix / num_classes)

        # Normalize so that each row sums to 1 (only necessary for multi-class)
        smoothed_labels /= np.sum(smoothed_labels, axis=-1, keepdims=True)

    return smoothed_labels


###############################################################################
#                                 Training Callbacks                           #
###############################################################################


# ✅ Single-task PredFOG-Only RareEventMetrics
class RareEventMetrics(tf.keras.callbacks.Callback):
    def __init__(self, validation_data):
        super().__init__()
        self.validation_data = validation_data

    def on_epoch_end(self, epoch, logs=None):
        y_true_predfog, y_pred_predfog = [], []

        for batch in self.validation_data:
            X_val, y_val = batch[0], batch[1]
            predictions = self.model.predict(X_val, verbose=0)
            pred_dict = dict(zip(self.model.output_names, predictions))

            predfog_pred = pred_dict['pred_fog_output']
            y_true = y_val['pred_fog_output'].numpy()

            y_true_predfog.append(y_true.flatten())
            y_pred_predfog.append(predfog_pred.flatten())

        y_true_predfog = np.hstack(y_true_predfog)
        y_pred_predfog = (np.hstack(y_pred_predfog) > 0.5).astype(int)

        prec = precision_score(y_true_predfog, y_pred_predfog, zero_division=0)
        rec = recall_score(y_true_predfog, y_pred_predfog, zero_division=0)
        f1 = f1_score(y_true_predfog, y_pred_predfog, zero_division=0)

        print(f"\nEpoch {epoch + 1} - PredFOG Output Metrics:")
        print(f"PredFOG - Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}")



class PerClassMetrics(tf.keras.callbacks.Callback):
    def __init__(self, validation_data, class_mapping, output_name):
        super().__init__()
        self.validation_data = validation_data
        self.class_mapping = class_mapping
        self.output_name = output_name

    def on_epoch_end(self, epoch, logs=None):
        # Skip deterministic outputs
        if self.output_name in ('posture_core_output', 'posture_transition_output'):
            print(f"[PerClassMetrics] Skipping non-trained head: {self.output_name}")
            return

        y_true_all, y_pred_all = [], []

        for x_val, y_val in self.validation_data:
            predictions = self.model.predict(x_val, verbose=0)
            pred_dict = dict(zip(self.model.output_names, predictions))

            if self.output_name not in pred_dict or self.output_name not in y_val:
                print(f"[PerClassMetrics] Output '{self.output_name}' missing from model or data.")
                continue

            pred = pred_dict[self.output_name]
            true = y_val[self.output_name].numpy()

            # --- Binary Classification
            if pred.shape[-1] == 1:
                y_pred = (pred > 0.5).astype(int).reshape(-1)
                y_true = true.reshape(-1)

            # --- Multi-Class Classification
            elif len(pred.shape) == 3:
                y_pred = pred.reshape(-1, pred.shape[-1]).argmax(axis=-1)
                if true.shape[-1] == pred.shape[-1]:
                    y_true = true.reshape(-1, true.shape[-1]).argmax(axis=-1)
                else:
                    y_true = true.reshape(-1)

            else:
                raise ValueError(f"[{self.output_name}] Unexpected shape: pred={pred.shape}, true={true.shape}")

            y_true_all.extend(y_true.tolist())
            y_pred_all.extend(y_pred.tolist())

        print(f"\n📊 [DEBUG] {self.output_name}: {len(y_true_all)} ground-truth | {len(y_pred_all)} predictions")

        try:
            print(f"\n📋 Classification Report for {self.output_name} (Epoch {epoch + 1}):")
            print(classification_report(
                y_true_all,
                y_pred_all,
                labels=list(self.class_mapping.keys()),
                target_names=[str(v) for v in self.class_mapping.values()],
                zero_division=0,
            ))

            acc = accuracy_score(y_true_all, y_pred_all)
            f1 = f1_score(y_true_all, y_pred_all, average='macro')
            print(f"✅ Accuracy: {acc:.4f} | Macro F1 Score: {f1:.4f}")
        except Exception as e:
            print(f"[ERROR] Failed to compute metrics for {self.output_name}: {e}")




# ✅ Multitask-Compatible EarlyStopping for Weak Class Targets
class CustomEarlyStoppingOnWeakClass(tf.keras.callbacks.Callback):
    def __init__(self, validation_data, target_class=1, patience=10, mode='f1', output_name='fog_output'):
        super().__init__()
        self.validation_data = validation_data
        self.target_class = target_class
        self.patience = patience
        self.wait = 0
        self.best_score = -np.Inf
        self.mode = mode  # 'f1' or 'recall'
        self.output_name = output_name

    def on_epoch_end(self, epoch, logs=None):
        # Skip non-learned heads
        if self.output_name in ('posture_core_output', 'posture_transition_output'):
            print(f"[CustomEarlyStopping] Skipping non-trained output: {self.output_name}")
            return

        y_true_all = []
        y_pred_all = []

        for batch in self.validation_data:
            x_batch, y_batch = batch[0], batch[1]

            predictions = self.model.predict(x_batch, verbose=0)
            pred_dict = dict(zip(self.model.output_names, predictions))

            if self.output_name not in pred_dict or self.output_name not in y_batch:
                print(f"[CustomEarlyStopping] Missing output: {self.output_name}")
                return

            y_pred_raw = pred_dict[self.output_name]
            y_true_raw = y_batch[self.output_name].numpy()

            # --- Binary classification
            if y_pred_raw.shape[-1] == 1:
                y_pred = (y_pred_raw > 0.5).astype(int).reshape(-1)
                y_true = y_true_raw.reshape(-1)

            # --- Multiclass (softmax)
            elif len(y_pred_raw.shape) == 3:
                y_pred = np.argmax(y_pred_raw, axis=-1).reshape(-1)
                if y_true_raw.shape[-1] == y_pred_raw.shape[-1]:
                    y_true = np.argmax(y_true_raw, axis=-1).reshape(-1)
                else:
                    y_true = y_true_raw.reshape(-1)

            else:
                raise ValueError(f"[CustomEarlyStopping] Unexpected shape for {self.output_name}")

            y_true_all.extend(y_true)
            y_pred_all.extend(y_pred)

        y_true_all = np.array(y_true_all)
        y_pred_all = np.array(y_pred_all)

        tp = np.sum((y_true_all == self.target_class) & (y_pred_all == self.target_class))
        fn = np.sum((y_true_all == self.target_class) & (y_pred_all != self.target_class))
        fp = np.sum((y_true_all != self.target_class) & (y_pred_all == self.target_class))

        recall = tp / (tp + fn + 1e-6)
        precision = tp / (tp + fp + 1e-6)
        f1 = 2 * precision * recall / (precision + recall + 1e-6)
        score = f1 if self.mode == 'f1' else recall

        print(f"\n📊 [CustomEarlyStopping] Epoch {epoch + 1} — {self.output_name} — "
              f"Class {self.target_class} {self.mode.upper()}: {score:.4f} (Best: {self.best_score:.4f})")

        if score > self.best_score:
            self.best_score = score
            self.wait = 0
        else:
            self.wait += 1
            print(f"⏳ No improvement — wait = {self.wait}/{self.patience}")
            if self.wait >= self.patience:
                print(f"🛑 Early stopping triggered on {self.output_name}, "
                      f"class {self.target_class}, based on {self.mode.upper()}.")
                self.model.stop_training = True


                
class EpochTrackerCallback(tf.keras.callbacks.Callback):
    """
    Updates .current_epoch on any provided loss instances.
    """
    def __init__(self, *losses, verbose=False):
        super().__init__()
        self.losses = losses
        self.verbose = verbose

    def on_epoch_begin(self, epoch, logs=None):
        for loss in self.losses:
            if hasattr(loss, "current_epoch"):
                loss.current_epoch = epoch
        print(f"🔄 [EpochTracker] Set epoch = {epoch} for {len(self.losses)} loss functions.")

        if self.verbose:
            for i, loss in enumerate(self.losses):
                print(f"🔄 Loss[{i}]: current_epoch updated to {epoch}")

            
            
            
# ✅ Multitask-Compatible EpochAccuracyF1Tracker
class EpochAccuracyF1Tracker(tf.keras.callbacks.Callback):
    def __init__(self, val_data, output_name='posture_output'):
        super().__init__()
        self.val_data = val_data
        self.output_name = output_name
        self.f1_scores = []
        self.accuracies = []
        self.epochs = []

    def on_epoch_end(self, epoch, logs=None):
        y_true_all = []
        y_pred_all = []

        for batch in self.val_data:
            x_batch, y_batch = batch[0], batch[1]

            predictions = self.model.predict(x_batch, verbose=0)
            pred_dict = dict(zip(self.model.output_names, predictions))
            y_pred = np.argmax(pred_dict[self.output_name], axis=-1).flatten()
            y_true = np.argmax(y_batch[self.output_name], axis=-1).flatten()

            y_true_all.extend(y_true)
            y_pred_all.extend(y_pred)

        acc = accuracy_score(y_true_all, y_pred_all)
        f1 = f1_score(y_true_all, y_pred_all, average='macro')

        self.accuracies.append(acc)
        self.f1_scores.append(f1)
        self.epochs.append(epoch)

        print(f"📊 Epoch {epoch+1}: Accuracy = {acc:.4f} | F1 (macro) = {f1:.4f}")


        
class WarmupLearningRateScheduler(tf.keras.callbacks.Callback):
    def __init__(self, warmup_epochs=5, init_lr=2e-4, target_lr=2e-4):
        super().__init__()
        self.warmup_epochs = warmup_epochs
        self.init_lr = init_lr
        self.target_lr = target_lr

    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.warmup_epochs:
            new_lr = self.init_lr * (epoch + 1) / self.warmup_epochs
            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)
            print(f"Warmup Epoch {epoch+1}: Setting learning rate to {new_lr:.6f}")
        elif epoch == self.warmup_epochs:
            tf.keras.backend.set_value(self.model.optimizer.lr, self.target_lr)
            print(f"Post-warmup: Learning rate stabilized at {self.target_lr:.6f}")


            
            
###############################################################################
#                             Dynamic Main Output Loss                         #
###############################################################################

class SimplifiedMainLoss(tf.keras.losses.Loss):
    def __init__(self, class_weights_dict, gamma=1.0, name="SimplifiedMainLoss", **kwargs):
        super().__init__(name=name, reduction=tf.keras.losses.Reduction.AUTO)

        # ✅ Normalize class weights
        max_weight = max(class_weights_dict.values())
        self.class_weights_dict = class_weights_dict
        self.class_weights = {
            cls: weight / max_weight for cls, weight in class_weights_dict.items()
        }

        self.gamma = gamma  # Focal loss parameter

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, dtype=tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-6, 1.0)  # Prevent log(0)

        # ✅ Map weights to class indices
        weights = tf.constant(
            [self.class_weights[i] for i in range(len(self.class_weights))],
            dtype=tf.float32
        )
        sample_weights = tf.reduce_sum(weights * y_true, axis=-1)

        # ✅ Cross-entropy and focal term
        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        pt = tf.stop_gradient(tf.reduce_sum(y_true * y_pred, axis=-1))
        focal_scaling = (1 - pt) ** self.gamma
        base_loss = sample_weights * focal_scaling * loss

        # 🛑 Extra penalty for False Positives on Class 1 (Standing)
        y_true_class = tf.argmax(y_true, axis=-1)
        y_pred_class = tf.argmax(y_pred, axis=-1)

        false_positive_class1 = tf.logical_and(
            tf.equal(y_pred_class, 1),  # Predicted as Class 1
            tf.not_equal(y_true_class, 1)  # But GT is NOT Class 1
        )
        fp_penalty_weight = 3.0  # Adjust this hyperparameter as needed

        # Create penalty mask and apply it
        penalty_mask = tf.cast(false_positive_class1, tf.float32)
        penalty_scaled_loss = base_loss + (penalty_mask * base_loss * (fp_penalty_weight - 1.0))

        return tf.reduce_sum(penalty_scaled_loss) / (tf.reduce_sum(sample_weights) + 1e-6)


    def get_config(self):
        return {
            "class_weights_dict": self.class_weights_dict,
            "gamma": self.gamma,
            "name": self.name
        }

    @classmethod
    def from_config(cls, config):
        return cls(**config)


def fog_weighted_categorical_loss(class_weights_dict=None,
                                  gamma=1.0,
                                  fp_penalty_classes=None,
                                  fp_penalty_weight=3.0,
                                  fp_penalty_dict=None):
    # Default false positive penalty classes
    if fp_penalty_classes is None:
        fp_penalty_classes = []

    def loss_fn(y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)  # Avoid log(0)
        print("y_true shape:", y_true.shape)

        # Automatically determine the number of classes from y_true
        num_classes = tf.shape(y_true)[-1]

        # Generate class weights tensor
        if class_weights_dict is not None:
            class_weights = [class_weights_dict.get(i, 1.0) for i in range(y_true.shape[-1])]
            max_weight = max(class_weights)
            class_weights = [w / max_weight for w in class_weights]
        else:
            class_weights = [1.0] * y_true.shape[-1]

        class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)

        # Compute class-weighted sample weights
        sample_weights = tf.reduce_sum(class_weights_tensor * y_true, axis=-1)

        # Apply external sample weights if provided
        if sample_weight is not None:
            sample_weight = tf.cast(sample_weight, tf.float32)
            sample_weights *= sample_weight

        # Standard categorical crossentropy loss
        ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)

        # Focal loss modulation
        pt = tf.stop_gradient(tf.reduce_sum(y_true * y_pred, axis=-1))
        focal_factor = tf.pow(1.0 - pt, gamma)
        loss = sample_weights * focal_factor * ce_loss

        # Optional false positive penalty
        y_true_class = tf.argmax(y_true, axis=-1)
        y_pred_class = tf.argmax(y_pred, axis=-1)
        penalty = tf.ones_like(loss)

        for cls in fp_penalty_classes:
            is_fp = tf.logical_and(tf.equal(y_pred_class, cls), tf.not_equal(y_true_class, cls))
            penalty_value = fp_penalty_dict.get(cls, fp_penalty_weight) if fp_penalty_dict else fp_penalty_weight
            penalty += tf.cast(is_fp, tf.float32) * (penalty_value - 1.0)

        loss *= tf.stop_gradient(penalty)

        # Fallback to large constant if NaN encountered
        final_loss = tf.reduce_mean(loss)
        return tf.where(tf.math.is_finite(final_loss), final_loss, tf.constant(1e6, dtype=final_loss.dtype))

    return loss_fn



# ✅ Multitask-Compatible FuzzyMatchMetrics
class FuzzyMatchMetrics(tf.keras.callbacks.Callback):
    def __init__(self, val_data, class_mapping, output_name='fog_output', tolerance=5):
        super().__init__()
        self.val_data = val_data
        self.class_mapping = class_mapping
        self.output_name = output_name
        self.tolerance = tolerance

    def on_epoch_end(self, epoch, logs=None):
        if self.output_name in ('posture_core_output', 'posture_transition_output'):
            print(f"[FuzzyMatchMetrics] Skipping non-trained output: {self.output_name}")
            return

        all_preds, all_labels = [], []

        for x_batch, y_batch in self.val_data:
            predictions = self.model.predict(x_batch, verbose=0)
            pred_dict = dict(zip(self.model.output_names, predictions))

            if self.output_name not in pred_dict or self.output_name not in y_batch:
                print(f"[FuzzyMatchMetrics] Output '{self.output_name}' missing from model or labels.")
                return

            y_pred_raw = pred_dict[self.output_name]
            y_true_raw = y_batch[self.output_name]

            # Multiclass shape: (batch, seq_len, classes)
            if len(y_pred_raw.shape) == 3 and y_pred_raw.shape[-1] > 1:
                y_pred = np.argmax(y_pred_raw, axis=-1)
                y_true = (np.argmax(y_true_raw, axis=-1) if y_true_raw.shape[-1] > 1
                          else y_true_raw)
            else:
                print(f"[FuzzyMatchMetrics] Only supports softmax outputs for now — skipping: {self.output_name}")
                return

            all_preds.append(y_pred.astype(int))
            all_labels.append(y_true.astype(int))

        preds = np.concatenate(all_preds).flatten()
        labels = np.concatenate(all_labels).flatten()

        fuzzy_preds = np.full_like(labels, fill_value=-1)

        for i in range(len(labels)):
            start = max(0, i - self.tolerance)
            end = min(len(preds), i + self.tolerance + 1)
            window_preds = preds[start:end]
            fuzzy_preds[i] = labels[i] if labels[i] in window_preds else preds[i]

        print(f"\n🟨 Epoch {epoch + 1} - Fuzzy Match (±{self.tolerance}) Per-Class F1 for {self.output_name}:")
        for cls_id, cls_name in self.class_mapping.items():
            cls_mask = (labels == cls_id)
            if not np.any(cls_mask):
                print(f"  • {cls_name} (Class {cls_id}): No samples in validation set.")
                continue
            true_binary = (labels == cls_id).astype(int)
            pred_binary = (fuzzy_preds == cls_id).astype(int)
            f1 = f1_score(true_binary, pred_binary, average='binary', zero_division=0)
            print(f"  • {cls_name} (Class {cls_id}): F1 = {f1:.4f}")

        macro_f1 = f1_score(labels, fuzzy_preds, average='macro', zero_division=0)
        print(f"🟡 Epoch {epoch + 1} - Fuzzy Macro F1 Score: {macro_f1:.4f}")




###############################################################################
#                               Data Reshaping                                #
################################################################### ############

def reshape_to_sequences(
    data,
    labels,
    sequence_length,
    num_features,
    labels_prefog=None,
    allow_partial_truncation=True
):
    """
    Reshape data and labels (optionally PreFOG) into fixed-length sequences.

    Parameters:
        data: (N, F)
        labels: (N, C) or (N, 1)
        sequence_length: int
        num_features: int
        labels_prefog: Optional (N, 1)
        allow_partial_truncation: If True, drops leftover frames

    Returns:
        reshaped_data: (num_sequences, sequence_length, num_features)
        reshaped_labels: (num_sequences, sequence_length, ...)
        reshaped_labels_prefog: (num_sequences, sequence_length, 1) if provided
    """

    total_samples = data.shape[0]
    num_sequences = total_samples // sequence_length
    trimmed_samples = num_sequences * sequence_length

    if not allow_partial_truncation and total_samples % sequence_length != 0:
        raise ValueError("Data length must be divisible by sequence_length unless allow_partial_truncation=True")

    # Truncate all
    data = data[:trimmed_samples].reshape(num_sequences, sequence_length, num_features)

    labels = labels[:trimmed_samples].reshape(num_sequences, sequence_length, -1)

    # Optional PreFOG
    reshaped_prefog = None
    if labels_prefog is not None:
        labels_prefog = labels_prefog[:trimmed_samples].reshape(num_sequences, sequence_length, 1)
        reshaped_prefog = labels_prefog

    return data, labels, reshaped_prefog




###############################################################################
#                             Normalization Functions                          #
###############################################################################

def tanh_normalization(data, save_scaler_path=None):
    """
    Apply tanh normalization feature-wise and optionally save parameters.
    """
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0) + 1e-6  # Avoid division by zero

    normalized_data = np.tanh((data - mean) / np.where(std > 1e-6, std, 1))

    if save_scaler_path:
        scaler_params = {'mean': mean, 'std': std}
        with open(save_scaler_path, 'wb') as f:
            pickle.dump(scaler_params, f)
        print(f"Tanh scaler parameters saved to {save_scaler_path}")

    return normalized_data, {'mean': mean, 'std': std}

def fit_and_save_scalers(
    input_data,
    feature_names,
    scaler_dir="scalers",
    scaler_map=None
):
    """
    Fits scalers to the input data and saves them along with feature names for later use.
    
    Args:
        input_data (np.ndarray): The input data to scale. Shape = (n_samples, n_features).
        feature_names (list of str): The names of the features (columns), length = n_features.
        scaler_dir (str): Directory to save the scaler files.
        scaler_map (dict): Maps a feature name to a scaler type. 
                           Example: {
                             'Velocity': 'robust',
                             'Acc': 'robust',
                             'Height': 'standard',
                             'FreezeInd': None,
                             ...
                           }
        
    Returns:
        np.ndarray: The scaled data (same shape as input_data).
    """
    
    # Sanity checks
    if input_data is None or input_data.size == 0:
        raise ValueError("Input data is empty or None. Cannot fit scalers.")
    
    if len(feature_names) != input_data.shape[1]:
        raise ValueError(
            f"Number of feature names ({len(feature_names)}) does not match "
            f"the number of columns in input_data ({input_data.shape[1]})."
        )
    
    print(f"Fitting scalers to input_data of shape: {input_data.shape}")
    
    # Default all features to standard scaler if scaler_map not provided
    if scaler_map is None:
        scaler_map = {fname: 'standard' for fname in feature_names}
    
    # Ensure output folder exists
    os.makedirs(scaler_dir, exist_ok=True)
    
    feature_scaler_map = {}  # Will map: feature_name -> path to the fitted scaler
    scaled_data = np.zeros_like(input_data, dtype=np.float32)
    
    for i, feature_name in enumerate(feature_names):
        scaler_choice = scaler_map.get(feature_name, 'standard')
        
        # If we choose to skip scaling for this feature
        if scaler_choice is None:
            # Just copy the values over without scaling
            scaled_data[:, i] = input_data[:, i]
            print(f"Feature '{feature_name}' was not scaled (None).")
            continue
        
        # Choose which scaler to use
        if scaler_choice.lower() == 'robust':
            scaler = RobustScaler()
        else:
            # Default to standard scaler
            scaler = StandardScaler()
        
        # Fit the scaler on this column
        col_data = input_data[:, i].reshape(-1, 1)
        scaled_col = scaler.fit_transform(col_data).flatten()
        
        # Store transformed data
        scaled_data[:, i] = scaled_col
        
        # Save the fitted scaler
        scaler_filename = os.path.join(scaler_dir, f"scaler_{feature_name}.pkl")
        with open(scaler_filename, "wb") as f:
            pickle.dump(scaler, f)
        
        feature_scaler_map[feature_name] = scaler_filename
        print(f"Scaler for feature '{feature_name}' saved to {scaler_filename} "
              f"(type = {scaler_choice}).")
    
    # Save the mapping of feature -> scaler file
    map_filename = os.path.join(scaler_dir, "feature_scaler_map.pkl")
    with open(map_filename, "wb") as f:
        pickle.dump(feature_scaler_map, f)
    print(f"Feature-scaler mapping saved to {map_filename}")
    
    return scaled_data

def transform_with_saved_scalers(input_data, feature_names, scaler_dir):
    num_features = input_data.shape[1]  # Number of features in the input data
    scaled_data = np.zeros_like(input_data)

    print(f"Number of features in input data: {num_features}")  # Should be 6

    for i, feature in enumerate(feature_names):
        if i < num_features:  # Ensure we do not exceed the number of available features
            scaler_file = os.path.join(scaler_dir, f"scaler_{feature}.pkl")
            if os.path.exists(scaler_file):
                scaler = joblib.load(scaler_file)
                print(f"Scaling {feature} with scaler from {scaler_file}")
                scaled_data[:, i] = scaler.transform(input_data[:, i].reshape(-1, 1)).flatten()
            else:
                print(f"Scaler for feature {feature} not found. Skipping.")
        else:
            print(f"Feature index {i} exceeds available number of features: {num_features}")
            break  # Stop if we've exceeded the available features

    return scaled_data


###############################################################################
#                       Model Saving & Conversion Functions                    #
###############################################################################

def convert_keras_to_onnx(model_path, onnx_model_path, sequence_length, input_dim, custom_objects):
    """
    Convert a Keras model (with custom objects) to ONNX format.
    """
    model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)
    spec = (tf.TensorSpec((None, sequence_length, input_dim), tf.float32, name="input"),)
    onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec)
    onnx.save_model(onnx_model, onnx_model_path)
    print(f"Model saved as '{onnx_model_path}'")
    
    
def convert_keras_to_onnx_strict(model_path, onnx_model_path, sequence_length, input_dim, custom_objects=None, opset=17):
    model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)

    # Build a concrete inference function with training=False
    @tf.function(input_signature=[tf.TensorSpec((None, sequence_length, input_dim), tf.float32, name="input")])
    def infer(x):
        return model(x, training=False)  # <-- KEY: force inference path


def save_trained_model(model, h5_path, saved_model_dir, keras_model_path, onnx_model_path, custom_objects, sequence_length, input_dim):
    """
    Save the trained model in various formats: .h5, .keras, TensorFlow SavedModel, and ONNX.
    """
    model.save(h5_path)
    model.save(keras_model_path)
    tf.keras.models.save_model(model, saved_model_dir, save_format='tf')

    #convert_keras_to_onnx(keras_model_path,onnx_model_path,sequence_length=sequence_length,input_dim=input_dim,custom_objects=custom_objects)
    
    convert_keras_to_onnx_strict(
        keras_model_path,
        onnx_model_path,
        sequence_length=sequence_length,
        input_dim=input_dim,
        custom_objects=custom_objects,
        opset=17
    )

    print(f"Model saved in ONNX format at: {onnx_model_path}")
    
    # Validate the ONNX model
    try:
        onnx.checker.check_model(onnx_model_path)
        print("ONNX model is valid.")
    except onnx.checker.ValidationError as e:
        print(f"ONNX model validation failed: {e}")

def convert_to_tflite(keras_model_path, tflite_model_path, custom_objects):
    """
    Convert a Keras model (with custom objects) to TFLite format.
    """
    os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # Use CPU for conversion

    model = tf.keras.models.load_model(keras_model_path, custom_objects=custom_objects)
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.allow_custom_ops = True
    tflite_model = converter.convert()

    with open(tflite_model_path, 'wb') as f:
        f.write(tflite_model)
    print(f"Model successfully converted to TFLite and saved at: {tflite_model_path}")

###############################################################################
#                         Post-Training Evaluation & Analysis                 #
###############################################################################

class ONNXModelWrapper:
    """
    A simple wrapper to load and predict with an ONNX model for further analysis.
    Handles both single-output (concatenated) and multi-output ONNX models.
    """
    def __init__(self, model_path):
        print(f"Initializing ONNXModelWrapper with model path: {model_path}")
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        print(f"ONNX Model Input Name: {self.input_name}")

    def predict(self, X):
        print(f"Input to predict() shape before adjustment: {X.shape}, dtype: {X.dtype}")
        # Ensure the input has the correct dimensions and data type
        if len(X.shape) == 2:
            X = X[:, np.newaxis, :]  # Add a time-step dimension if missing
            print(f"Adjusted input shape to add time dimension: {X.shape}")
        X = X.astype(np.float32)
        print(f"Input to predict() after dtype adjustment: {X.shape}, dtype: {X.dtype}")

        try:
            # Run inference using the ONNX session
            preds = self.session.run(None, {self.input_name: X})
            if len(preds) == 2:
                # If two outputs, return them directly (main and FOG predictions)
                print(f"ONNX Predictions Shapes: {[pred.shape for pred in preds]}")
                return preds[0], preds[1]
            elif len(preds) == 1:
                # If single output, assume it's concatenated and split
                concatenated_output = preds[0]
                onnx_main = concatenated_output[:, :, :-1]  # Main predictions
                onnx_fog = concatenated_output[:, :, -1:]   # FOG predictions
                print(f"ONNX Main Output Shape: {onnx_main.shape}")
                print(f"ONNX FOG Output Shape: {onnx_fog.shape}")
                return onnx_main, onnx_fog
            else:
                # Unexpected number of outputs
                raise ValueError(f"Unexpected number of outputs: {len(preds)}")
        except Exception as e:
            print(f"Error during ONNX prediction: {e}")
            raise e

def multiclass_metric(y_true, y_pred):
    """
    Accuracy score for multi-class classification, used for permutation importance.
    """
    if len(y_true.shape) > 1 and y_true.shape[1] > 1:
        y_true_class = y_true.argmax(axis=1)
    else:
        y_true_class = y_true
    y_pred_class = y_pred.argmax(axis=1)
    return accuracy_score(y_true_class, y_pred_class)

def binary_metric(y_true, y_pred):
    """
    Accuracy score for binary classification, used for permutation importance.
    """
    if y_pred.ndim == 3:
        y_pred_avg = y_pred.mean(axis=1).ravel()
    elif y_pred.ndim == 2:
        y_pred_avg = y_pred.mean(axis=1)
    else:
        raise ValueError(f"Invalid shape for y_pred: {y_pred.shape}")
    y_pred_class = (y_pred_avg > 0.5).astype(int)
    return accuracy_score(y_true, y_pred_class)

def permutation_importance(wrapper, X, y, metric, n_repeats=10, aggregate_axis=1, threshold=0.01):
    """
    Calculate permutation feature importance for a model with a specified metric.
    Handles ONNX multi-output predictions (main and FOG separately).
    
    Args:
        wrapper: ONNXModelWrapper instance for inference.
        X: Input data.
        y: Ground truth labels.
        metric: Metric function (e.g., accuracy).
        n_repeats: Number of permutations per feature.
        aggregate_axis: Axis along which to aggregate predictions for the metric.
        threshold: Threshold for significance in feature importance.
    
    Returns:
        mean_importances: Mean importance for each feature.
        significant_importances: Boolean array indicating significant features.
    """
    # Predict with ONNX model
    onnx_predictions = wrapper.predict(X)
    if isinstance(onnx_predictions, (list, tuple)):
        # Assume the first output is the main output
        y_pred_main = onnx_predictions[0]
    else:
        raise ValueError("Unexpected ONNX output format. Expected a tuple or list.")

    # Aggregate predictions for comparison (e.g., mean over time)
    y_pred_aggregated = y_pred_main.mean(axis=aggregate_axis)
    base_score = metric(y, y_pred_aggregated)

    num_features = X.shape[-1]
    importances = np.zeros((n_repeats, num_features))

    for col in range(num_features):
        for repeat in range(n_repeats):
            # Shuffle a single feature column
            X_shuffled = X.copy()
            np.random.shuffle(X_shuffled[:, :, col])  # shuffle in-place

            # Predict with shuffled data
            shuffled_predictions = wrapper.predict(X_shuffled)
            if isinstance(shuffled_predictions, (list, tuple)):
                shuffled_main = shuffled_predictions[0]
            else:
                raise ValueError("Unexpected ONNX output format after shuffling.")

            # Aggregate predictions for metric calculation
            shuffled_aggregated = shuffled_main.mean(axis=aggregate_axis)
            shuffled_score = metric(y, shuffled_aggregated)

            # Calculate importance as drop in score
            importances[repeat, col] = base_score - shuffled_score

    # Calculate mean importances and determine significance
    mean_importances = importances.mean(axis=0)
    significant_importances = np.abs(mean_importances) > threshold

    return mean_importances, significant_importances

def plot_feature_importances(importances, feature_names, title="Feature Importances"):
    """
    Plot bar chart of feature importances (descending order).
    """
    sorted_idx = np.argsort(importances)[::-1]
    sorted_importances = importances[sorted_idx]
    sorted_feature_names = [feature_names[i] for i in sorted_idx]

    plt.figure(figsize=(10, 6))
    plt.barh(sorted_feature_names, sorted_importances, color='skyblue')
    plt.xlabel("Importance")
    plt.title(title)
    plt.gca().invert_yaxis()
    plt.show()

def get_class_counts(labels_main):
    """
    Count frequency of each class in one-hot encoded labels_main.
    """
    flat_labels = labels_main.reshape(-1, labels_main.shape[-1])
    class_counts = Counter(flat_labels.argmax(axis=1))
    total_samples = sum(class_counts.values())
    class_frequencies = {cls: count / total_samples for cls, count in class_counts.items()}

    print("\nClass Counts:")
    for cls, count in class_counts.items():
        print(f"Class {cls}: Count = {count}")

    print("\nClass Frequencies:")
    for cls, freq in class_frequencies.items():
        print(f"Class {cls}: Frequency = {freq:.4f}")

    return class_counts, class_frequencies

def clip_outliers_featurewise(data, percentile=95):
    """
    Clips outliers feature-wise based on the specified percentile.
    Each feature is treated independently.
    """
    clipped_data = np.copy(data)  # Avoid modifying the original array
    for i in range(data.shape[1]):  # Iterate over each feature
        lower_bound = np.percentile(data[:, i], 100 - percentile)
        upper_bound = np.percentile(data[:, i], percentile)
        clipped_data[:, i] = np.clip(data[:, i], lower_bound, upper_bound)
    return clipped_data


def pt_biserial(labels, features):
    """
    Computes point-biserial correlation while handling constant features and mismatched lengths.
    Supports multiple features without flattening everything into 1D.
    """

    # Ensure 1D array for labels
    labels = np.asarray(labels, dtype=np.int32).flatten()

    # Ensure features is a 2D array (samples, num_features)
    features = np.asarray(features, dtype=np.float64)

    if features.ndim == 1:
        features = features.reshape(-1, 1)  # Convert (samples,) → (samples, 1)

    num_samples, num_features = features.shape

    # Ensure labels and features have matching number of samples
    if len(labels) != num_samples:
        print(f"⚠️ Length mismatch: Labels({len(labels)}) vs Features({num_samples} samples, {num_features} features). Truncating.")
        min_length = min(len(labels), num_samples)
        labels = labels[:min_length]
        features = features[:min_length, :]

    # Compute correlation for each feature independently
    correlation_results = []
    for i in range(num_features):
        feature = features[:, i]
        
        # Skip if feature has only one unique value (constant)
        unique_values = np.unique(feature)
        if len(unique_values) <= 1:
            print(f"🚨 Skipping Feature_{i} - constant values: {unique_values}")
            correlation_results.append(np.nan)
            continue

        correlation_value, _ = stats.pointbiserialr(labels, feature)
        correlation_results.append(correlation_value)

    # If single feature, return a scalar instead of an array
    return correlation_results[0] if num_features == 1 else np.array(correlation_results)


def extract_and_analyze(input_train, labels_main_train, input_val, labels_main_val, motor_states, motor_state_indices, generate_plots=False):
    """
    Analyzes data, performs statistical comparisons, and calculates point-biserial correlation.
    Now operates on non-sequenced data.
    """

    # Debug prints for dataset shapes
    print("Inside extract_and_analyze()")
    print(f"Train Features Shape (before squeeze): {input_train.shape}")
    print(f"Train Labels Shape: {labels_main_train.shape}")
    print(f"Validation Features Shape (before squeeze): {input_val.shape}")
    print(f"Validation Labels Shape: {labels_main_val.shape}")

    # Ensure proper feature shape (remove redundant dimensions)
    input_train = np.squeeze(input_train)  # Converts (N, 1, 6) → (N, 6) if needed
    input_val = np.squeeze(input_val)

    print(f"Train Features Shape (after squeeze): {input_train.shape}")
    print(f"Validation Features Shape (after squeeze): {input_val.shape}")

    # Ensure labels match feature dimensions
    assert input_train.shape[0] == labels_main_train.shape[0], "Train features and labels mismatch"
    assert input_val.shape[0] == labels_main_val.shape[0], "Validation features and labels mismatch"

    # Results dictionaries
    pt_biserial_results_train = {}
    pt_biserial_results_val = {}
    feature_contributions_train = {}
    feature_contributions_val = {}

    num_features = input_train.shape[1]  # Ensure 2D feature data

    # Iterate through motor states
    for state in motor_states:
        motor_state_idx = motor_state_indices[state]

        # Extract binary labels for this motor state (2D → 1D)
        labels_for_state_train = labels_main_train[:, motor_state_idx]
        labels_for_state_val = labels_main_val[:, motor_state_idx]

        # Debugging: Unique values and label distribution
        print(f"\n{state} - Label Distribution: Train(1s: {np.sum(labels_for_state_train)}, 0s: {len(labels_for_state_train) - np.sum(labels_for_state_train)}) | Validation(1s: {np.sum(labels_for_state_val)}, 0s: {len(labels_for_state_val) - np.sum(labels_for_state_val)})")

        # Skip calculation for nearly constant or insufficiently variable labels
        if len(np.unique(labels_for_state_train)) <= 1:
            print(f"⚠️ Skipping {state} in training data: constant labels.")
            continue
        if len(np.unique(labels_for_state_val)) <= 1:
            print(f"⚠️ Skipping {state} in validation data: constant labels.")
            continue

        # Compute pt-biserial correlation
        pt_biserial_results_train[state] = pt_biserial(labels_for_state_train, input_train)
        pt_biserial_results_val[state] = pt_biserial(labels_for_state_val, input_val)

        # Store feature-wise correlations
        feature_contributions_train[state] = {
            f"Feature_{i}": pt_biserial_results_train[state][i] for i in range(num_features)
        }
        feature_contributions_val[state] = {
            f"Feature_{i}": pt_biserial_results_val[state][i] for i in range(num_features)
        }

    # Summary printout
    print("\n------------------------------")
    print("Summary of Point-Biserial Correlation Results")
    print("------------------------------")

    # Sort and display results
    for dataset, results in [("Training", pt_biserial_results_train), ("Validation", pt_biserial_results_val)]:
        print(f"\n{dataset} Results - pt_biserial correlation (Sorted by |Correlation|):")
        sorted_states = sorted(results.keys(), key=lambda x: np.max(np.abs(results[x])), reverse=True)
        for state in sorted_states:
            sorted_features = sorted(enumerate(results[state]), key=lambda x: abs(x[1]), reverse=True)
            print(f"{state}: {[(f'Feature_{idx}', round(val, 4)) for idx, val in sorted_features]}")

    return (
        pt_biserial_results_train,
        pt_biserial_results_val,
        feature_contributions_train,
        feature_contributions_val
    )

def calculate_individual_pt_biserial(features, labels, motor_states):
    """
    Calculate pt-biserial correlation for each feature individually.
    """
    feature_correlations = {}
    for feature_name, feature_data in features.items():
        feature_correlations[feature_name] = {}
        for state in motor_states:
            motor_state_idx = motor_state_indices[state]
            labels_for_state = labels[:, motor_state_idx]
            correlation_value = pt_biserial(labels_for_state, feature_data)
            feature_correlations[feature_name][state] = correlation_value
            print(f"{feature_name} - {state} pt-biserial correlation: {correlation_value}")
    return feature_correlations


def calculate_ensemble_stats(summary_data):
    ensemble_stats = {}

    # Loop through each feature in the summary data
    for feature_name, all_feature_data in summary_data.items():
        print(f"{feature_name} has {len(all_feature_data)} entries")

        # Ensure there is data for the feature (non-empty array)
        if all_feature_data.size > 0:  # Check if the array is not empty
            # If 2D, flatten the array to 1D for analysis
            if all_feature_data.ndim > 1:
                all_feature_data = all_feature_data.flatten()

            ensemble_mean = np.mean(all_feature_data)
            ensemble_std = np.std(all_feature_data)
            ensemble_range = np.max(all_feature_data) - np.min(all_feature_data)

            ensemble_stats[feature_name] = {
                "mean": ensemble_mean,
                "std": ensemble_std,
                "range": ensemble_range,
                "outliers": []  # Assuming no outlier logic for now
            }

            print(f"{feature_name} - Mean: {ensemble_mean}, Std: {ensemble_std}, Range: {ensemble_range}")
        else:
            print(f"No data available for feature: {feature_name}. Skipping.")

    return ensemble_stats


def compare_feature_distributions(train_features, val_features):
    # Plotting distribution comparisons (histograms, boxplots)
    import matplotlib.pyplot as plt
    import seaborn as sns

    feature_names = ["Velocity_XZ", "Height_Above_Ground", "CamCorrected_X", "CamCorrected_Z", "CamCorrectedAngle"]

    for i, feature_name in enumerate(feature_names):
        plt.figure(figsize=(10, 6))
        sns.histplot(train_features[:, i], kde=True, label='Train', color='blue', alpha=0.5)
        sns.histplot(val_features[:, i], kde=True, label='Val', color='orange', alpha=0.5)
        plt.title(f'Distribution Comparison for {feature_name} (Train vs Validation)')
        plt.xlabel(feature_name)
        plt.ylabel('Frequency')
        plt.legend()
        plt.show()


def validate_data_alignment(val_data, val_labels_main, val_labels_prefog, sequence_length):
    val_data_sequences, val_labels_sequences_main = reshape_to_sequences(
        val_data, val_labels_main, sequence_length, val_data.shape[-1]
    )
    
    val_labels_sequences_prefog = reshape_to_sequences(
        val_data, val_labels_prefog, sequence_length, val_data.shape[-1]
    )[1]  # Extracting the label sequences for PreFOG

    # Ensure the number of sequences match between data and labels
    assert val_data_sequences.shape[0] == val_labels_sequences_main.shape[0], "Mismatch in validation data and main labels!"
    assert val_data_sequences.shape[0] == val_labels_sequences_prefog.shape[0], "Mismatch in validation data and PreFOG labels!"

    return val_data_sequences, val_labels_sequences_main, val_labels_sequences_prefog


def relaxed_recall(y_true, y_pred, tolerance=3):
    """
    Compute recall while allowing a ±tolerance frame shift.
    """
    y_true = K.cast(y_true, dtype='float32')  # Ensure correct dtype
    y_pred = K.cast(K.round(y_pred), dtype='float32')  # Convert predictions to binary

    # Convert `y_true` to boolean before reducing with `tf.reduce_any()`
    expanded_true = tf.reduce_any(
        tf.stack([tf.roll(tf.cast(y_true, tf.bool), shift=i, axis=1) for i in range(-tolerance, tolerance + 1)]),
        axis=0
    )

    # Compute relaxed recall
    relaxed_correct = K.sum(y_pred * K.cast(expanded_true, dtype='float32'), axis=1)
    
    # ✅ Fix: Ensure `y_true` sum is float32 before division
    recall = relaxed_correct / (K.cast(K.sum(y_true, axis=1), dtype='float32') + K.epsilon())  
    
    return K.mean(recall)


def remove_short_transitions(state_array, min_duration=10):
    """
    Removes transitions shorter than `min_duration` frames.
    Reassigns short-lived transitions to the previous or next stable state.
    """
    cleaned_state = state_array.copy()
    indices = np.where(state_array == 1)[0]  # Get indices of active states
    
    if len(indices) == 0:
        return cleaned_state
    
    runs = np.split(indices, np.where(np.diff(indices) > 1)[0] + 1)
    
    for run in runs:
        if len(run) < min_duration:  # Remove if shorter than threshold
            prev_state = cleaned_state[run[0] - 1] if run[0] > 0 else 0
            next_state = cleaned_state[run[-1] + 1] if run[-1] < len(cleaned_state) - 1 else 0
            new_state = prev_state if prev_state != 0 else next_state  # Assign to nearest stable state
            cleaned_state[run] = new_state
    
    return cleaned_state


def remove_short_turns(turn_array, velocity, min_turn_duration=10):
    """
    Removes short turn segments and assigns them to walking or standing based on velocity.
    """
    cleaned_turn = turn_array.copy()
    indices = np.where(turn_array != 0)[0]
    
    if len(indices) == 0:
        return cleaned_turn
    
    runs = np.split(indices, np.where(np.diff(indices) > 1)[0] + 1)
    
    for run in runs:
        if len(run) < min_turn_duration:
            avg_velocity = np.mean(velocity[run])  # Check movement level
            new_state = 1 if avg_velocity > 0.25 else 0  # Assign to walking (1) or standing (0)
            cleaned_turn[run] = new_state
    
    return cleaned_turn

def max_continuous_length(labels, target_class):
    """
    Finds the longest continuous segment of a given class in the provided labels.
    
    Args:
        labels (numpy array): Array of categorical labels (1D or 2D one-hot encoded).
        target_class (int): The class label to analyze.
    
    Returns:
        int: Maximum continuous duration of the target class.
    """
    if labels.ndim == 2:  # One-hot encoded, convert to categorical indices
        labels = labels.argmax(axis=-1)
    
    # Identify where `target_class` appears
    target_indices = np.where(labels == target_class)[0]
    
    if len(target_indices) == 0:
        return 0  # No occurrences of the target class
    
    # Compute segment lengths
    max_length = 0
    current_length = 1
    
    for i in range(1, len(target_indices)):
        if target_indices[i] == target_indices[i - 1] + 1:
            current_length += 1
        else:
            max_length = max(max_length, current_length)
            current_length = 1
    
    return max(max_length, current_length)  # Return longest found


def print_feature_stats(val_data, test_data, feature_names=None, threshold=0.2):
    # Flatten to 2D if needed (e.g., [n_samples, seq_len, n_features] → [-1, n_features])
    if val_data.ndim > 2:
        val_data = val_data.reshape(-1, val_data.shape[-1])
        test_data = test_data.reshape(-1, test_data.shape[-1])

    n_features = val_data.shape[-1]
    feature_names = feature_names or [f"Feature_{i}" for i in range(n_features)]
    report = []
    flagged = []

    for i in range(n_features):
        val_feat, test_feat = val_data[:, i], test_data[:, i]
        val_mean, val_std = np.mean(val_feat), np.std(val_feat)
        test_mean, test_std = np.mean(test_feat), np.std(test_feat)
        report.append({
            'Feature': feature_names[i],
            'Val Mean': val_mean, 'Val Std': val_std,
            'Train Mean': test_mean, 'Train Std': test_std,
            'Δ Mean': abs(val_mean - test_mean),
            'Δ Std': abs(val_std - test_std)
        })
        if abs(val_mean - test_mean) > threshold or abs(val_std - test_std) > threshold:
            flagged.append(feature_names[i])

    df = pd.DataFrame(report)
    df['Flagged'] = df['Feature'].isin(flagged)
    return df


def print_class_distribution(one_hot_labels, name):
    dominant_classes = np.argmax(one_hot_labels, axis=-1)
    if dominant_classes.ndim == 2:
        dominant_classes = [np.argmax(np.bincount(seq)) for seq in dominant_classes]
    counts = Counter(dominant_classes)
    print(f"\n{name} Class Distribution:")
    for cls, count in sorted(counts.items()):
        print(f"  Class {cls}: {count}")

def print_fog_overlap_by_class(main_labels, fog_mask, class_mapping=None):
    main_classes = np.argmax(main_labels, axis=-1)
    fog_mask_flat = fog_mask.squeeze(-1)
    
    overlap_counts = defaultdict(int)
    for cls in np.unique(main_classes):
        overlap = np.logical_and(main_classes == cls, fog_mask_flat == 1)
        overlap_counts[cls] = int(np.sum(overlap))

    print("\n🔍 FOG Overlap by Motor Class:")
    for cls, count in overlap_counts.items():
        name = class_mapping[cls] if class_mapping else f"Class {cls}"
        print(f"{name:15}: {count} frames with FOG")
        
def plot_pbc_comparison(pbc_before, pbc_after, pbc_val, feature_names=None, figsize=(12, 6)):
    """
    Visualize PBC comparisons for each class across Train (Before), Train (After Oversampling), and Val.

    Parameters:
    - pbc_before: dict of class -> np.array (before oversampling)
    - pbc_after: dict of class -> np.array (after oversampling)
    - pbc_val: dict of class -> np.array (val set)
    - feature_names: list of feature names (optional)
    - figsize: tuple, size of each plot
    """
    all_classes = list(pbc_before.keys())

    for cls in all_classes:
        before = pbc_before[cls]
        after = pbc_after[cls]
        val = pbc_val[cls]

        n_features = len(before)
        x = np.arange(n_features)

        plt.figure(figsize=figsize)
        width = 0.25

        plt.bar(x - width, before, width, label="Train (Before)", alpha=0.7)
        plt.bar(x, after, width, label="Train (After)", alpha=0.7)
        plt.bar(x + width, val, width, label="Val", alpha=0.7)

        plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)
        plt.title(f"PBC Comparison: {cls}")
        plt.ylabel("Correlation with Feature")
        plt.xticks(x, feature_names if feature_names else [f"F{i}" for i in range(n_features)], rotation=45)
        plt.legend()
        plt.grid(axis='y', linestyle='--', alpha=0.5)
        plt.tight_layout()
        plt.show()

        
def plot_f1_and_accuracy(tracker):
    plt.figure(figsize=(10, 5))
    plt.plot(tracker.epochs, tracker.f1_scores, label="F1 Score (Macro)", marker='o')
    plt.plot(tracker.epochs, tracker.accuracies, label="Accuracy", marker='s')
    plt.xlabel("Epoch")
    plt.ylabel("Score")
    plt.title("Validation Accuracy and F1 over Epochs")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("f1_acc_plot.png", dpi=300)
    plt.show()
    

def simple_weighted_categorical_loss(class_weights_dict):
    sorted_weights = [class_weights_dict[i] for i in sorted(class_weights_dict.keys())]
    weights = tf.constant(sorted_weights, dtype=tf.float32)

    def loss_fn(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)

        sample_weights = tf.reduce_sum(weights * y_true, axis=-1)
        sample_weights = tf.clip_by_value(sample_weights, 0.1, 3.0)

        ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        weighted_loss = ce_loss * sample_weights
        final_loss = tf.reduce_mean(weighted_loss)

        return tf.where(tf.math.is_finite(final_loss), final_loss, tf.constant(1e6, dtype=final_loss.dtype))

    return loss_fn

def sustain_gate(mask, min_frames):
    gate = np.zeros_like(mask)
    labeled, n = label(mask)
    for i in range(1, n + 1):
        indices = np.where(labeled == i)[0]
        if len(indices) >= min_frames:
            gate[indices] = 1
    return gate.astype(bool)

def smooth_binary(binary_array, window_size=9):
    return uniform_filter1d(binary_array.astype(float), size=window_size)

def mask_long_enough(signal: np.ndarray, min_len: int = 15):
    signal = signal.astype(int)
    out = np.zeros_like(signal)
    labeled, n = label(signal)
    for i in range(1, n + 1):
        idx = np.where(labeled == i)[0]
        if len(idx) >= min_len:
            out[idx] = 1
    return out

def pad_binary_regions(signal: np.ndarray, pre_pad: int = 5, post_pad: int = 5):
    padded = signal.copy()
    ones = np.where(signal == 1)[0]
    for idx in ones:
        padded[max(0, idx - pre_pad): idx + post_pad + 1] = 1
    return padded

def compute_displacement_signal(x, y, window_size=30, smooth=True, smoothing_size=15):
    coords = np.stack([x, y], axis=-1)
    disp = np.full(len(x), np.nan)
    for i in range(window_size, len(x)):
        path = coords[i - window_size:i + 1]
        disp[i] = np.sum(np.linalg.norm(np.diff(path, axis=0), axis=1))
    if smooth:
        disp = np.nan_to_num(disp)
        disp = uniform_filter1d(disp, size=smoothing_size, mode='nearest')
    return disp

def clean_displacement(disp, threshold=3.0):
    disp = np.array(disp)
    disp[np.abs(disp) > threshold] = np.nan
    return disp

def interpolate_nan(disp):
    isnan = np.isnan(disp)
    not_nan = ~isnan
    disp[isnan] = np.interp(np.flatnonzero(isnan), np.flatnonzero(not_nan), disp[not_nan])
    return disp

def smooth_displacement(disp, window=15):
    return uniform_filter1d(disp, size=window, mode='nearest')

def process_displacement_signal(disp_raw, clip_thresh=3.0, smooth_win=15, aggressive=True):
    disp = clean_displacement(disp_raw, threshold=clip_thresh)
    disp = interpolate_nan(disp)
    disp = smooth_displacement(disp, window=smooth_win)
    if aggressive:
        # Use percentile to define reasonable upper bound (e.g., 99.5th percentile)
        upper = np.percentile(disp, 99.5)
        median = np.median(disp)
        # Apply adaptive log compression to all values above median
        disp = np.where(disp > median, median + np.log1p(disp - median) / np.log(3), disp)

        # Optionally hard cap above extreme threshold
        disp = np.clip(disp, 0, upper)
    return disp

###############################################################################
#                                Main Pipeline                                #
###############################################################################

def simple_pipeline(folder_path, sequence_length=30, num_augmentations=1, param_grid=None):
    """
    A high-level pipeline that:
      1. Loads and prepares data from .mat/.hdf5 files.
      2. Normalizes and augments data (both feature-wise and label smoothing).
      3. Builds a multi-output LSTM model (main output + FOG).
      4. Trains the model, logs metrics, and saves in multiple formats.
      5. Demonstrates how to perform ONNX inference + permutation importance.
    """
    
    class_mapping = {
        0: 'Sitting',
        1: 'Standing',
        2: 'SitToStand',
        3: 'StandToSit',
        4: 'Turning_InPlace_L',
        5: 'Turning_InPlace_R',
        6: 'Turning_Walking_L',
        7: 'Turning_Walking_R',
        8: 'Walking',
        9: 'Gait Init',
        10: 'Gait Term',
        11: 'FOG',
        12: 'PredFOG'
    }

    # ✅ Class Frequency Counts (Derived Empirically)
    class_counts = {
        0: 73225, 1: 11057, 2: 8969, 3: 7122,
        4: 19967, 5: 10589, 6: 33037, 7: 21583,
        8: 75708, 9: 5820, 10: 3364, 11: 68917, 12: 86341
    }
    

    # ✅ Step 1: Smoothed Inverse Frequency (base weight)
    max_count = max(class_counts.values())
    initial_weights = {
        cls: 1.0 + (max_count - count) / max_count
        for cls, count in class_counts.items()
    }

    # ✅ Step 2: Apply log-scaling to reduce extremes
    max_log_clip = 2.0
    log_scaled_weights = {
        k: min(max_log_clip, np.log1p(v))
        for k, v in initial_weights.items()
    }

    # ✅ Step 3: Optional Per-Class Tuning (mild multipliers)
    critical_class_multipliers = {
        0: 0.85,    # Slightly lower for overrepresented Sitting
        1: 1.5,     # More help for underdetected Standing
        2: 1.4,     # SitToStand needs stronger push
        3: 1.0,     # Fine
        4: 1.1,     # Slight encouragement for TIP_L
        5: 1.1,     # TIP_R
        6: 1.05,    # TW_L
        7: 1.05,    # TW_R
        8: 0.9,     # Keep slightly suppressed
        9: 1.1,     # Gait Init — may be misclassified
        10: 1.2,    # Gait Term — rare and underdetected
        11: 1.2,     # FOG — current sweet spot
        12: 1.2     # predFOG 
        
    }


    # ✅ Step 4: Apply tuning multipliers
    tuned_weights = {
        cls: log_scaled_weights[cls] * critical_class_multipliers.get(cls, 1.0)
        for cls in class_counts
    }

    # ✅ Step 5: Cap final values for stability
    max_allowed_weight = 2.25
    class_weights_dict = {
        cls: min(weight, max_allowed_weight)
        for cls, weight in tuned_weights.items()
    }

    # ✅ Final Class Weights for Verification
    print("\n🔹 Final Class Weights (Smoothed, Tuned, and Capped):")
    for c in sorted(class_weights_dict):
        print(f"Class {c} ({class_mapping[c]}): Weight = {class_weights_dict[c]:.4f}")


        
    class_smoothing_factors = {
        2: 0.5,    # SitToStand: transitional → moderate inertia helps capture full motion
        3: 0.5,    # StandToSit: same as above

        4: 0.15,    # Turn in Place L
        5: 0.15,    # Turn in Place R
        # Turning in place is often short and noisy — this helps ensure coherence

        6: 0.15,    # Turn While Walk L
        7: 0.15,    # Turn While Walk R
        # Walking turns can be short; avoid over-smoothing but still provide temporal stability

        9: 0.1,    # Gait Init: should trigger briefly, no smoothing
        10: 0.1,   # Gait Term: same — needs fast response

        11: 0.2    # FOG: prevent brief blips; enforce stability
    }

      
#     fog_class_smoothing_factors = {1: 0.01}  
        
    # ✅ **Prepare Data**
    train_folder = "ml_outliers_removed/splits/train/"
    val_folder = "ml_outliers_removed/splits/val/"

    (
        train_data,
        train_labels_main,
        train_labels_prefog,
        val_data,
        val_labels_main,
        val_labels_prefog,
        train_ensemble_stats,
        val_ensemble_stats,
        train_summary_data,
        val_summary_data,
        train_seated_heights,
        val_seated_heights,
        train_standing_heights,
        val_standing_heights,
    ) = prepare_data(
        train_folder=train_folder,
        val_folder=val_folder,
        class_mapping=class_mapping,
        generate_plots=True
    )
    
    # 🔧 Ensure PreFOG labels are strictly binary
    train_labels_prefog = np.clip(train_labels_prefog, 0, 1)
    val_labels_prefog = np.clip(val_labels_prefog, 0, 1)
    print("Unique FOG label values (train):", np.unique(train_labels_prefog))
    print("Unique FOG label values (val):", np.unique(val_labels_prefog))

    # Pick all standing and walking frames across the dataset
    standing_mask = (np.argmax(train_labels_main, axis=-1) == 1).flatten()
    walking_mask  = (np.argmax(train_labels_main, axis=-1) == 8).flatten()

    # Flatten input data to (total_frames, features)
    X_flat = train_data.reshape(-1, train_data.shape[-1])

    # Calculate averages
    mean_standing = np.mean(X_flat[standing_mask], axis=0)
    mean_walking  = np.mean(X_flat[walking_mask], axis=0)

    print("🔍 Feature means (Standing):", mean_standing)
    print("🔍 Feature means (Walking):", mean_walking)
    print("🧪 Feature differences:", mean_walking - mean_standing)


    # ✅ Sanity checks
    assert train_data.shape[0] == train_labels_main.shape[0], "Train mismatch!"
    assert val_data.shape[0] == val_labels_main.shape[0], "Val mismatch!"

    # ✅ Key shapes
    print(f"\n✅ Train Data: {train_data.shape}, Labels: {train_labels_main.shape}")
    print(f"✅ Validation Data: {val_data.shape}, Labels: {val_labels_main.shape}")

    # ✅ Feature stats (if relevant)
    if train_ensemble_stats and val_ensemble_stats:
        for feat in train_ensemble_stats:
            print(f"📊 {feat} | Train μ={train_ensemble_stats[feat]['mean']:.3f}, σ={train_ensemble_stats[feat]['std']:.3f}, "
                  f"Outliers={len(train_ensemble_stats[feat]['outliers'])} | "
                  f"Val μ={val_ensemble_stats[feat]['mean']:.3f}, σ={val_ensemble_stats[feat]['std']:.3f}, "
                  f"Outliers={len(val_ensemble_stats[feat]['outliers'])}")

    # ✅ NaN / Inf check
    print(f"🔍 NaNs in Train: {np.isnan(train_data).sum()}, Val: {np.isnan(val_data).sum()}")
    print(f"🔍 Infs in Train: {np.isinf(train_data).sum()}, Val: {np.isinf(val_data).sum()}")

    # ✅ Label distribution
    flattened_train_labels = train_labels_main.argmax(axis=-1).ravel()
    flattened_val_labels = val_labels_main.argmax(axis=-1).ravel()
    print("\n📊 Class Distribution - Main (Train):", dict(enumerate(np.bincount(flattened_train_labels))))
    print("📊 Class Distribution - Main (Val):", dict(enumerate(np.bincount(flattened_val_labels))))

    # ✅ PreFOG distribution
    train_prefog_counts = np.bincount(train_labels_prefog.flatten(), minlength=2)
    val_prefog_counts = np.bincount(val_labels_prefog.flatten(), minlength=2)
    print("\n🔍 PreFOG Label Distribution (Train):", dict(enumerate(train_prefog_counts)))
    print("🔍 PreFOG Label Distribution (Val):", dict(enumerate(val_prefog_counts)))
    print(f"📊 PreFOG Ratio (Train): {train_prefog_counts[1]/sum(train_prefog_counts):.4f}, "
          f"(Val): {val_prefog_counts[1]/sum(val_prefog_counts):.4f}")


    ############ FEATURE ENGINEERING ##########################
    
    # Feature Engineering - Training
    print("Applying feature eng to training data...")
    train_data_augmented, train_labels_main_augmented = feat_eng(
        train_data, train_labels_main, train_labels_prefog 
    )
    

    # Feature Engineering - Val/Test
    print("Applying feat eng to validation data...")
    val_data_augmented, val_labels_main_augmented = feat_eng(
        val_data, val_labels_main, val_labels_prefog
    )
    
    ######################################    

    train_labels_prefog_augmented = train_labels_prefog
    val_labels_prefog_augmented = val_labels_prefog
    

    ############### FEATURE STATS #################################################
    

    feature_stats_df = print_feature_stats(val_data_augmented, train_data_augmented)
    
    
    ############ PBC & CORRELATION ANALYSIS ##########################


    # ✅ Flatten for optional full correlation matrix
    train_data_augmented_flattened = train_data_augmented.reshape(-1, train_data_augmented.shape[-1])
    val_data_flattened = val_data_augmented.reshape(-1, val_data_augmented.shape[-1])

    # Optional full correlation (commented out unless needed)
    # correlation_matrix_train = np.corrcoef(train_data_augmented_flattened, rowvar=False)

    # 🔍 Analyze correlations with motor states
    motor_states = ['Sitting', 'Standing', 'SitToStand', 'StandToSit', 'TurnL1', 'TurnR1', 'TurnL2', 'TurnR2', 'Walking', 'Gait_Init', 'Gait_Term', 'FOG', 'predFOG']
    motor_state_indices = {state: i for i, state in enumerate(motor_states)}

    ptb_train, ptb_val, _, _ = extract_and_analyze(
        train_data_augmented, train_labels_main_augmented,
        val_data_augmented, val_labels_main_augmented,
        motor_states, motor_state_indices,
        generate_plots=False
    )

    print("\n🔍 pt-biserial correlation: Motor States")
    print(f"Train:\n{ptb_train}\nVal:\n{ptb_val}")

    # 🔍 Analyze PreFOG correlations
    ptb_train_prefog, ptb_val_prefog, _, _ = extract_and_analyze(
        train_data_augmented, train_labels_prefog_augmented,
        val_data_augmented, val_labels_prefog_augmented,
        ['predFOG'], {'predFOG': 0},
        generate_plots=False
    )

    print("\n🔍 pt-biserial correlation: PreFOG")
    print(f"Train:\n{ptb_train_prefog}\nVal:\n{ptb_val_prefog}")

    # 📊 Final Label Distribution Summary
#     print("\n📊 Label Distributions (Flattened):")
#     print("Train - Main:", Counter(train_labels_main_augmented.argmax(axis=-1).flatten()))
#     print("Train - PreFOG:", Counter(train_labels_prefog_augmented.flatten()))
#     print("Val   - Main:", Counter(val_labels_main_augmented.argmax(axis=-1).flatten()))
#     print("Val   - PreFOG:", Counter(val_labels_prefog_augmented.flatten()))
    
    ############### SEQUENCE SEGMENTATION ################

#     print("Val Labels (Main) — Pre-reshape label 11 count:", np.sum(val_labels_main[:, 11]))
    print("✅ Check: Class 12 count in train_labels_main_augmented:", np.sum(train_labels_main_augmented[..., 12]))
    print("✅ Check: Class 12 count in val_labels_main_augmented:", np.sum(val_labels_main_augmented[..., 12]))

    # 🧱 Sequence segmentation
    sequence_length = 30

    print("🔄 Reshaping training and validation data into sequences...")

    # Drop extra dimension if needed
    if train_data_augmented.shape[1] == 1:
        train_data_augmented = np.squeeze(train_data_augmented, axis=1)
    if val_data_augmented.shape[1] == 1:
        val_data_augmented = np.squeeze(val_data_augmented, axis=1)

    # --- Main Inputs ---
    reshaped_input_train, reshaped_labels_main_train, _ = reshape_to_sequences(
        data=train_data_augmented,
        labels=train_labels_main_augmented,
        sequence_length=sequence_length,
        num_features=train_data_augmented.shape[-1],
        allow_partial_truncation=True
    )

    reshaped_input_val, reshaped_labels_main_val, _ = reshape_to_sequences(
        data=val_data_augmented,
        labels=val_labels_main_augmented,
        sequence_length=sequence_length,
        num_features=val_data_augmented.shape[-1],
        allow_partial_truncation=True
    )

    # --- preFOG binary labels: true binary GT, NOT class 12 ---
    _, reshaped_labels_prefog_train, _ = reshape_to_sequences(
        data=train_labels_prefog_augmented,
        labels=train_labels_prefog_augmented,
        sequence_length=sequence_length,
        num_features=1,
        allow_partial_truncation=True
    )

    _, reshaped_labels_prefog_val, _ = reshape_to_sequences(
        data=val_labels_prefog_augmented,
        labels=val_labels_prefog_augmented,
        sequence_length=sequence_length,
        num_features=1,
        allow_partial_truncation=True
    )

    # ✅ Use only classes 0–10 for auxiliary motor label logic (posture, locomotion, turning, etc.)
    motor_labels_train = np.argmax(reshaped_labels_main_train[..., :11], axis=-1)
    motor_labels_val   = np.argmax(reshaped_labels_main_val[..., :11], axis=-1)


    # ✅ preFOG binary mask from separate binary GT label
    prefog_bin_train = tf.convert_to_tensor(reshaped_labels_prefog_train.astype(np.float32))
    prefog_bin_val = tf.convert_to_tensor(reshaped_labels_prefog_val.astype(np.float32))

    # ✅ Verifications
    print("✅ preFOG (GT) frames - Train:", int(tf.reduce_sum(prefog_bin_train).numpy()),
          "| Val:", int(tf.reduce_sum(prefog_bin_val).numpy()))


    # ──────────────────────────────────────────────────────────────────────────────
    # 1)  OUTPUT-HEAD DEFINITIONS
    # ──────────────────────────────────────────────────────────────────────────────

    output_configs = [

        ('locomotion_output',         {6:1, 7:1, 8:1},               2, None),
        ('turn_direction_output',     {4:1, 6:1, 5:2, 7:2},          3, None),
        ('pred_fog_output',           None,                          2, None),
    ]

    name_maps = {

        'locomotion_output'        : {0:'NonWalk', 1:'Walk'},
        'turn_direction_output'    : {0:'None', 1:'Left', 2:'Right'},
        'pred_fog_output'          : {0:'NoPreFOG', 1:'PreFOG'},
    }


    def build_map(overrides, n_base=13):
        m = {i: 0 for i in range(n_base)}
        if overrides: m.update(overrides)
        return m

    flat = np.argmax(reshaped_labels_main_val, axis=-1).flatten()
    print("Class 12 count after reshaping:", np.sum(flat == 12))

    print("motor_labels_val shape:", motor_labels_val.shape)
    print("motor_labels_val class 12 count:", np.sum(motor_labels_val == 12))

    main_sample_weight_train = np.ones_like(motor_labels_train, dtype=float)
    main_sample_weight_val   = np.ones_like(motor_labels_val,   dtype=float)

    train_labels_flat = motor_labels_train.flatten()
    val_labels_flat   = motor_labels_val.flatten()

    train_class_counts = {int(cls): int((train_labels_flat == cls).sum()) for cls in np.unique(train_labels_flat)}
    val_class_counts   = {int(cls): int((val_labels_flat   == cls).sum()) for cls in np.unique(val_labels_flat)}

    print("\nClass Distribution in Training:")
    for cls, count in sorted(train_class_counts.items()):
        print(f"  Class {cls:<2}: {count:>6} frames")

    print("\nClass Distribution in Validation:")
    for cls, count in sorted(val_class_counts.items()):
        print(f"  Class {cls:<2}: {count:>6} frames")

    print("\nPre-FOG Binary Mask Counts (sum of 1s):")
    print(f"  Pre-FOG → Train: {int(tf.reduce_sum(prefog_bin_train).numpy()):>5} | Val: {int(tf.reduce_sum(prefog_bin_val).numpy()):>5}")


    # ──────────────────────────────────────────────────────────────────────────────
    # 3) BUILD LABEL & WEIGHT DICTIONARIES (for active output heads only)
    # ──────────────────────────────────────────────────────────────────────────────
    labels_train_dict, labels_val_dict = {}, {}
    sample_weights_train_dict, sample_weights_val_dict = {}, {}

    # Iterate over all output heads (excluding removed ones)
    for head, mapping, n_cls, _ in output_configs:

        # --- LABELS ---
        if head == 'pred_fog_output':
            labels_train_dict[head] = tf.cast(prefog_bin_train, tf.int32)
            labels_val_dict[head]   = tf.cast(prefog_bin_val, tf.int32)
#             labels_train_dict[head] = tf.cast(prefog_bin_train > 0.1, tf.int32)
#             labels_val_dict[head]   = tf.cast(prefog_bin_val > 0.1, tf.int32)

        else:
            mp = build_map(mapping)
            tr = np.vectorize(mp.get)(motor_labels_train)
            va = np.vectorize(mp.get)(motor_labels_val)

            labels_train_dict[head] = tf.convert_to_tensor(tr, dtype=tf.int32)
            labels_val_dict[head]   = tf.convert_to_tensor(va, dtype=tf.int32)

        # --- SAMPLE WEIGHTS ---
        if head in ('pred_fog_output'):
            lbls_tr = labels_train_dict[head].numpy().reshape(-1)
            lbls_va = labels_val_dict[head].numpy().reshape(-1)

            pos_weight_tr = (len(lbls_tr) - lbls_tr.sum()) / (lbls_tr.sum() + 1e-6)
            pos_weight_va = (len(lbls_va) - lbls_va.sum()) / (lbls_va.sum() + 1e-6)

            sw_tr = np.where(lbls_tr == 1, pos_weight_tr, 1.0).reshape(labels_train_dict[head].shape)
            sw_va = np.where(lbls_va == 1, pos_weight_va, 1.0).reshape(labels_val_dict[head].shape)

        else:
            sw_tr, sw_va = main_sample_weight_train, main_sample_weight_val

        sample_weights_train_dict[head] = tf.convert_to_tensor(sw_tr, dtype=tf.float32)
        sample_weights_val_dict[head]   = tf.convert_to_tensor(sw_va, dtype=tf.float32)


    # ──────────────────────────────────────────────────────────────────────────────
    # Class Priors → Used for Bias Initializers
    # ──────────────────────────────────────────────────────────────────────────────

    ml_flat = motor_labels_train.reshape(-1).astype(np.int32)
    priors  = np.bincount(ml_flat, minlength=13) / ml_flat.size

    print("\n📊 Class Priors:")
    for i, p in enumerate(priors):
        print(f"  Class {i:2}: Prior = {p:.6f}")

    def binit(p): return np.log((p + 1e-8) / (1 - p + 1e-8))


    # ──────────────────────────────────────────────────────────────────────────────
    # Bias Initializers for ACTIVE HEADS ONLY
    # ──────────────────────────────────────────────────────────────────────────────

    # Locomotion (Walking vs Non-Walking)
    walking_prior = priors[[6, 7, 8]].sum()

    # Turn Direction (No Turn, Left, Right)
    turn_dir_priors = [
        priors[[0, 1, 2, 8]].sum(),  # No Turn
        priors[[4, 6]].sum(),        # Turn Left
        priors[[5, 7]].sum(),        # Turn Right
    ]

    prefog_prior  = priors[12]

    # Optional: floor/clip priors to prevent extreme logits
    #prefog_prior = np.clip(prefog_prior, 0.15, 0.85) # ~ logit -1.4 to +1.7
    prefog_prior = np.clip(prefog_prior, 0.00, 1.0) # ~ logit -1.4 to +1.7

    # Recompute logits
    def binit(p): 
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    bias_inits = {
        'locomotion_output'     : tf.keras.initializers.Constant(binit(walking_prior)),
        'turn_direction_output' : tf.keras.initializers.Constant([binit(p) for p in turn_dir_priors]),
        'pred_fog_output'       : tf.keras.initializers.Constant(binit(prefog_prior)),
    }

    # ──────────────────────────────────────────────────────────────────────────────
    # 5) tf.data datasets (for model inputs, training, and metrics eval)
    # ──────────────────────────────────────────────────────────────────────────────

    batch_size = 16

    # --- Training and Validation datasets with sample weights
    train_dataset = tf.data.Dataset.from_tensor_slices(
        (reshaped_input_train.astype('float32'),
         labels_train_dict,
         sample_weights_train_dict)
    ).batch(batch_size, drop_remainder=True)

    val_dataset = tf.data.Dataset.from_tensor_slices(
        (reshaped_input_val.astype('float32'),
         labels_val_dict,
         sample_weights_val_dict)
    ).batch(batch_size)

    # --- Metrics labels (for computing metrics after prediction)
    metrics_labels_val_dict = {
        'pred_fog_output': labels_val_dict['pred_fog_output'],
    }

    # Add one-hot encoded labels for other heads
    for head, _, n_cls, _ in output_configs:
        if head in ('pred_fog_output'):
            continue

        # All other heads: convert to one-hot
        oh = tf.keras.utils.to_categorical(labels_val_dict[head], num_classes=n_cls)
        metrics_labels_val_dict[head] = oh

    # --- Final metrics dataset
    metrics_val_dataset = tf.data.Dataset.from_tensor_slices(
        (reshaped_input_val.astype('float32'), metrics_labels_val_dict)
    ).batch(batch_size)

    # --- Special subset: just fog-related labels
    fog_val_dataset = tf.data.Dataset.from_tensor_slices(
        (reshaped_input_val.astype('float32'),
         {
             'pred_fog_output': metrics_labels_val_dict['pred_fog_output']
         })
    ).batch(batch_size)

    # ──────────────────────────────────────────────────────────────────────────────
    # 6)  Build model
    # ──────────────────────────────────────────────────────────────────────────────
    num_features = reshaped_input_train.shape[-1]
    model = build_lstm_model(
        input_shape=(sequence_length, num_features),
        output_units_locomotion      = 2,
        output_units_turn_dir        = 3,
        output_units_predfog         = 1,
        dropout_rate                 = 0.4,
        bias_initializers            = bias_inits,
    )

    # ──────────────────────────────────────────────────────────────────────────────
    # 7)  Losses & optimiser
    # ──────────────────────────────────────────────────────────────────────────────

    max_epochs = 120

    # Learning Rate Schedule with Warmup + Cosine Decay
    lr_sched = tf.keras.optimizers.schedules.CosineDecay(
        initial_learning_rate=2e-4,
        decay_steps=max_epochs - 10,  # stretch cosine curve a bit longer
        alpha=1e-5 / 2e-4
    )

    sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy()

    # Slightly higher weight to discourage false positives
    class WeightedBinaryCrossentropy(tf.keras.losses.Loss):
        def __init__(self, pos_weight=4.75, reduction=tf.keras.losses.Reduction.AUTO, name="weighted_binary_crossentropy"):
            super().__init__(reduction=reduction, name=name)
            self.pos_weight = pos_weight

        def call(self, y_true, y_pred):
            epsilon = tf.keras.backend.epsilon()
            y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
            y_true = tf.cast(y_true, y_pred.dtype)
            pos_weight_tensor = tf.constant(self.pos_weight, dtype=y_pred.dtype)

            log_loss = -(pos_weight_tensor * y_true * tf.math.log(y_pred) +
                         (1.0 - y_true) * tf.math.log(1.0 - y_pred))
            return tf.reduce_mean(log_loss)
        
    class FocalBinaryCrossentropy(tf.keras.losses.Loss):
        def __init__(self, gamma=2.0, pos_weight=1.0, reduction=tf.keras.losses.Reduction.AUTO, name="focal_binary_crossentropy", **kwargs):
            super().__init__(reduction=reduction, name=name, **kwargs)
            self.gamma = gamma
            self.pos_weight = pos_weight

        def call(self, y_true, y_pred):
            epsilon = tf.keras.backend.epsilon()
            y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
            y_true = tf.cast(y_true, y_pred.dtype)

            pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
            weight = tf.where(tf.equal(y_true, 1), self.pos_weight, 1.0)

            loss = -weight * tf.pow(1.0 - pt, self.gamma) * tf.math.log(pt)
            return tf.reduce_mean(loss)

        def get_config(self):
            config = super().get_config()
            config.update({
                "gamma": self.gamma,
                "pos_weight": self.pos_weight,
            })
            return config


    # Focal Loss: More emphasis on difficult Right and Left classes
    softmax_focal_turn_dir = SoftmaxFocalLoss(
        gamma=1.75,  # ⬆ focus on hard misclassified examples
        class_weights=[1.0, 9.0, 9.0]  # slight bump to None class too
    )
    
    loss_dict = {
        'locomotion_output'     : sparse_ce,
        'turn_direction_output' : softmax_focal_turn_dir,
        'pred_fog_output'       : FocalBinaryCrossentropy(gamma=1.75, pos_weight=4.5)
    }

    metrics_dict = {
        'locomotion_output'     : ['accuracy'],
        'turn_direction_output' : ['accuracy'],
        'pred_fog_output'       : ['accuracy']
    }

    loss_weights = {
        'locomotion_output'     : 2.0,
        'turn_direction_output' : 10.0,
        'pred_fog_output'       : 8.0
    }


    # Compile
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_sched, clipnorm=1.0),
        loss=loss_dict,
        loss_weights=loss_weights,
        metrics=metrics_dict
    )

    # ──────────────────────────────────────────────────────────────────────────────
    # 8) Callbacks
    # ──────────────────────────────────────────────────────────────────────────────

    class PredictionHistogram(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs=None):
            outs = dict(zip(self.model.output_names,
                            self.model(reshaped_input_val[:1024], training=False)))
            if 'pred_fog_output' in outs:
                print(f"[Hist] pred_fog_output μ={outs['pred_fog_output'].numpy().mean():.3f}")

    # Print validation pre-FOG frame counts
    pref_flat = labels_val_dict['pred_fog_output'].numpy().reshape(-1)
    print("Pre-FOG (combined) frames in validation set:", int(pref_flat.sum()))
    
    
    class FreezeLocomotionHead(tf.keras.callbacks.Callback):
        def __init__(self, freeze_epoch=15, loss=None, metrics=None):
            super().__init__()
            self.freeze_epoch = freeze_epoch
            self.loss = loss
            self.metrics = metrics
            self.frozen = False

        def on_epoch_begin(self, epoch, logs=None):
            if not self.frozen and epoch == self.freeze_epoch:
                print(f"[FreezeLocomotionHead] Freezing locomotion_output layers at epoch {epoch}")
                for layer in self.model.layers:
                    if "locomotion_output" in layer.name:
                        layer.trainable = False

                self.model.compile(
                    optimizer=self.model.optimizer,
                    loss=self.loss,
                    metrics=self.metrics
                )
                self.model.make_train_function()
                self.frozen = True

    # Callback setup
    callbacks = [
        PredictionHistogram(),
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),
        RareEventMetrics(validation_data=fog_val_dataset),  # Ensure this callback is compatible
        FreezeLocomotionHead(
            freeze_epoch=15,
            loss=loss_dict,
            metrics=metrics_dict
        ),
    ]

    for head, _, _, es_target in output_configs:
        if head not in name_maps:
            continue
        cmap = name_maps[head]
        callbacks.append(PerClassMetrics(metrics_val_dataset, cmap, head))
        if es_target is not None:
            callbacks.append(CustomEarlyStoppingOnWeakClass(
                validation_data=metrics_val_dataset,
                target_class=es_target,
                patience=30,
                mode='f1',
                output_name=head
            ))

    print(f"Training on {reshaped_input_train.shape[0]} sequences; "
          f"validating on {reshaped_input_val.shape[0]}.")

    # ──────────────────────────────────────────────────────────────────────────────
    # 9)  Train & save
    # ──────────────────────────────────────────────────────────────────────────────
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=max_epochs,
        callbacks=callbacks
    )

    custom_objects = {
        "SoftmaxFocalLoss": SoftmaxFocalLoss,
#         "WeightedBinaryCrossentropy": WeightedBinaryCrossentropy,
        "FocalBinaryCrossentropy": FocalBinaryCrossentropy
    }

    save_trained_model(
        model,
        "best_model.h5",
        "saved_model_directory",
        "best_model.keras",
        "model.onnx",
        custom_objects=custom_objects,
        sequence_length=sequence_length,
        input_dim=num_features
    )

    # ✅ Convert to TFLite
    convert_to_tflite("best_model.keras", "best_model.tflite", custom_objects)

    print(f"✅ Model saved with input shape: {model.input_shape} and outputs: ['locomotion_output', 'turn_direction_output', 'pred_fog_output']")


# Run the pipeline
if __name__ == "__main__":
    folder_path = "ml_outliers_removed/"  # Adjust as needed
    simple_pipeline(folder_path)
